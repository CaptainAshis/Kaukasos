{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 870M (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to import Vgg16 as well because I'll want it's low-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# sys.path.insert(1, os.path.join('../utils/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, looks like Vgg's ImageNet weights won't be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from vgg16 import Vgg16\n",
    "# vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Preprocessing\n",
    "Keras Convolutional layers expect color channels, so expand an empty dimension in the input data, to account for no colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.expand_dims(x_train, 1) # can also enter <axis=1> for <1>\n",
    "x_test = np.expand_dims(x_test, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this notebook's models are all mimicking Vgg16, the input data should be preprocessed in the same way: in this case normalized by subtracting the mean and dividing by the standard deviation. It turns out this is a good idea generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_mean = x_train.mean().astype(np.float32)\n",
    "x_stdv = x_train.std().astype(np.float32)\n",
    "def norm_input(x): return (x - x_mean) / x_stdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Data Batch Generator\n",
    "```ImageDataGenerator``` with no arguments will return a generator. Later, when data is augmented, it'll be told how to do so. I don't know what batch-size should be set to: in Lecture it was 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "trn_batches = gen.flow(x_train, y_train, batch_size=64)\n",
    "tst_batches = gen.flow(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General workflow, going forward:\n",
    "* Define the model's architecture.\n",
    "* Run 1 Epoch at default learning rate (0.01 ~ 0.001 depending on optimizer) to get it started.\n",
    "* Jack up the learning to 0.1 (as high as you'll ever want to go) and run 1 Epoch, possibly more if you can get away with it.\n",
    "* Lower the learning rate by a factor of 10 and run for a number of Epochs -- repeat until model begins to overfit (acc > valacc)\n",
    "\n",
    "Points on internal architecture:\n",
    "* Each model will have a data-preprocessing ```Lambda``` layer, which normalizes the input and assigns a shape of (1 color-channel x 28 pixels x 28 pixels)\n",
    "* Weights are flattened before entering FC layers\n",
    "* Convolutional Layers will come in 2 pairs (because this is similar to the Vgg model). \n",
    "* Convol layer-pairs will start with 32 3x3 filters and double to 64 3x3 layers\n",
    "* A MaxPooling Layer comes after each Convol-pair.\n",
    "* When Batch-Normalization is applied, it is done after every layer but last (excluding MaxPooling).\n",
    "* Final layer is always an FC softmax layer with 10 outputs for our 10 digits.\n",
    "* Dropout, when applied, should increase toward later layers.\n",
    "* Optimizer used in Lecture was Adam(), all layers but last use a ReLU activation, loss function is categorical cross-entropy.\n",
    "\n",
    "### 1. Linear Model\n",
    "aka 'Dense', 'Fully-Connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LinModel():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1, 28, 28)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WayNoxchi/Miniconda3/envs/FAI/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_5 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 21s - loss: 0.4237 - acc: 0.8734 - val_loss: 0.3017 - val_acc: 0.9143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x111b44690>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear_model = LinModel()\n",
    "Linear_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1,\n",
    "                          validation_data=tst_batches, nb_val_samples=trn_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 12s - loss: 0.2972 - acc: 0.9162 - val_loss: 0.2886 - val_acc: 0.9182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x112838790>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear_model.optimizer.lr=0.1\n",
    "Linear_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1,\n",
    "                          validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 16s - loss: 0.2843 - acc: 0.9199 - val_loss: 0.2712 - val_acc: 0.9250\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 11s - loss: 0.2769 - acc: 0.9212 - val_loss: 0.2854 - val_acc: 0.9246\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 11s - loss: 0.2698 - acc: 0.9240 - val_loss: 0.2893 - val_acc: 0.9211\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 11s - loss: 0.2708 - acc: 0.9243 - val_loss: 0.2820 - val_acc: 0.9197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1128388d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear_model.optimizer.lr=0.01\n",
    "Linear_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=4,\n",
    "                          validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 16s - loss: 0.2648 - acc: 0.9255 - val_loss: 0.2776 - val_acc: 0.9217\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 13s - loss: 0.2612 - acc: 0.9265 - val_loss: 0.2699 - val_acc: 0.9249\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 11s - loss: 0.2649 - acc: 0.9265 - val_loss: 0.2766 - val_acc: 0.9237\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 13s - loss: 0.2563 - acc: 0.9292 - val_loss: 0.2867 - val_acc: 0.9227\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 11s - loss: 0.2586 - acc: 0.9283 - val_loss: 0.2894 - val_acc: 0.9208\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 13s - loss: 0.2561 - acc: 0.9286 - val_loss: 0.2790 - val_acc: 0.9237\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 18s - loss: 0.2564 - acc: 0.9289 - val_loss: 0.2861 - val_acc: 0.9233\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 17s - loss: 0.2548 - acc: 0.9292 - val_loss: 0.2733 - val_acc: 0.9269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1088a2ed0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear_model.optimizer.lr=0.001\n",
    "Linear_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=8,\n",
    "                          validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Single Dense Layer\n",
    "This is what people in the 80s & 90s thought of as a 'Neural Network': a single Fully-Connected hidden layer. I don't yet know why the hidden layer is ouputting 512 units. For natural-image recognition it's 4096. I'll see whether a ReLU or Softmax hidden layer works better.\n",
    "\n",
    "By the way, the training and hyper-parameter tuning process *should* be automated. I want to use a NN to figure out how to do that for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FCModel():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1, 28, 28)),\n",
    "        Dense(512, activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WayNoxchi/Miniconda3/envs/FAI/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_7 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s - loss: 0.2062 - acc: 0.9385 - val_loss: 0.1386 - val_acc: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113c7ce10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC_model = FCModel()\n",
    "FC_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1,\n",
    "                      validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 36s - loss: 0.1188 - acc: 0.9644 - val_loss: 0.1266 - val_acc: 0.9637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113e6bbd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC_model.optimizer=0.1\n",
    "FC_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1,\n",
    "                      validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 44s - loss: 0.0971 - acc: 0.9707 - val_loss: 0.1309 - val_acc: 0.9630\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 34s - loss: 0.0819 - acc: 0.9748 - val_loss: 0.1187 - val_acc: 0.9651\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 33s - loss: 0.0679 - acc: 0.9783 - val_loss: 0.1309 - val_acc: 0.9661\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 29s - loss: 0.0568 - acc: 0.9823 - val_loss: 0.1212 - val_acc: 0.9664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113e7cd90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC_model.optimizer=0.01\n",
    "FC_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=4,\n",
    "                      validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an accuracy of 0.9823 and validation accuracy of 0.9664, the model's starting to overfit significantly and hit its limits, so it's time to go on to the next technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic 'VGG' style Convolutional Neural Network\n",
    "\n",
    "I'm specifying an output shape equal to the input shape, to suppress the warnings keras was giving me; and it stated it was defaulting to that anyway. Or maybe I should've written ```output_shape=input_shape```\n",
    "\n",
    "Aha: yes it's as I thought. See [this thread](http://forums.fast.ai/t/warning-output-shape-argument-not-specified/416/10) -- output_shape warnings were added to Keras, and neither vgg16.py (nor I until now) were specifying output_shape. It's fine.\n",
    "\n",
    "The first time I ran this, I forgot to have 2 pairs of Conv layers. At the third λr=0.01 epoch I had acc/val of 0.9964, 0.9878\n",
    "\n",
    "Also noticing: in lecture JH was using a GPU which I think was an NVidia Titan X. I'm using an Intel Core i5 CPU on a MacBook Pro. His epochs took on average 6 seconds, mine are taking 180~190. Convolutions are also the most computationally-intensive part of the NN being built here.\n",
    "\n",
    "Interestingly, the model with 2 Conv-layer pairs is taking avg 160s. Best Acc/Val: ```0.9968/0.9944```\n",
    "\n",
    "Final: ```0.9975/0.9918``` - massive overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvModel():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1, 28, 28), output_shape=(1, 28, 28)),\n",
    "        Convolution2D(32, 3, 3, activation='relu'),\n",
    "        Convolution2D(32, 3, 3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64, 3, 3, activation='relu'),\n",
    "        Convolution2D(64, 3, 3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 168s - loss: 0.1039 - acc: 0.9682 - val_loss: 0.0385 - val_acc: 0.9866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x123ed46d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model = ConvModel()\n",
    "CNN_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1,\n",
    "                       validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 174s - loss: 0.0352 - acc: 0.9896 - val_loss: 0.0449 - val_acc: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124513250>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.optimizer=0.1\n",
    "CNN_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                       validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 157s - loss: 0.0246 - acc: 0.9926 - val_loss: 0.0267 - val_acc: 0.9925\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 159s - loss: 0.0185 - acc: 0.9940 - val_loss: 0.0276 - val_acc: 0.9895\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 158s - loss: 0.0160 - acc: 0.9949 - val_loss: 0.0312 - val_acc: 0.9903\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 157s - loss: 0.0133 - acc: 0.9958 - val_loss: 0.0228 - val_acc: 0.9926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124513150>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.optimizer=0.01\n",
    "CNN_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=4, verbose=1,\n",
    "                       validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 166s - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0209 - val_acc: 0.9944\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 153s - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0299 - val_acc: 0.9931\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 156s - loss: 0.0081 - acc: 0.9977 - val_loss: 0.0384 - val_acc: 0.9907\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 158s - loss: 0.0075 - acc: 0.9975 - val_loss: 0.0316 - val_acc: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124513210>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running again until validation accuracy stops increasing\n",
    "CNN_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=4, verbose=1,\n",
    "                       validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                           height_shift_range=0.08, zoom_range=0.08)\n",
    "trn_batches = gen.flow(x_train, y_train, batch_size=64)\n",
    "tst_batches = gen.flow(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 171s - loss: 0.2000 - acc: 0.9364 - val_loss: 0.0700 - val_acc: 0.9773\n",
      "Learning Rate, η = 0.1\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 168s - loss: 0.0693 - acc: 0.9781 - val_loss: 0.0491 - val_acc: 0.9848\n",
      "Learning Rate, η = 0.01\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 164s - loss: 0.0550 - acc: 0.9829 - val_loss: 0.0407 - val_acc: 0.9872\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 172s - loss: 0.0460 - acc: 0.9858 - val_loss: 0.0433 - val_acc: 0.9858\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 165s - loss: 0.0424 - acc: 0.9872 - val_loss: 0.0395 - val_acc: 0.9874\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 169s - loss: 0.0397 - acc: 0.9878 - val_loss: 0.0339 - val_acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122656110>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_Aug_model = ConvModel()\n",
    "CNN_Aug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                  validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "# upping LR\n",
    "print(\"Learning Rate, η = 0.1\")\n",
    "CNN_Aug_model.optimizer.lr=0.1\n",
    "CNN_Aug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                  validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "# brining LR back down for more epochs\n",
    "print(\"Learning Rate, η = 0.01\")\n",
    "CNN_Aug_model.optimizer.lr=0.01\n",
    "CNN_Aug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=4, verbose=1,\n",
    "                  validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 167s - loss: 0.0378 - acc: 0.9885 - val_loss: 0.0336 - val_acc: 0.9899\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 170s - loss: 0.0346 - acc: 0.9891 - val_loss: 0.0393 - val_acc: 0.9884\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 169s - loss: 0.0317 - acc: 0.9902 - val_loss: 0.0295 - val_acc: 0.9913\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 169s - loss: 0.0308 - acc: 0.9910 - val_loss: 0.0326 - val_acc: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1245137d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 more epochs at η=0.01\n",
    "CNN_Aug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=4, verbose=1,\n",
    "                  validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Batch Normalization + Data Augmentation\n",
    "\n",
    "[See this thread](http://forums.fast.ai/t/batchnormalization-axis-1-when-used-on-convolutional-layers/214) for info on BatchNorm axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvModelBN():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1, 28, 28), output_shape=(1, 28, 28)),\n",
    "        Convolution2D(32, 3, 3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(32, 3, 3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(64, 3, 3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(64, 3, 3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 769s - loss: 0.1630 - acc: 0.9487 - val_loss: 0.0842 - val_acc: 0.9739\n",
      "Learning Rate, η = 0.1\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 2210s - loss: 0.0724 - acc: 0.9770 - val_loss: 0.0504 - val_acc: 0.9845\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 323s - loss: 0.0569 - acc: 0.9823 - val_loss: 0.0435 - val_acc: 0.9866\n",
      "Learning Rate, η = 0.01\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 323s - loss: 0.0536 - acc: 0.9830 - val_loss: 0.0396 - val_acc: 0.9881\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 322s - loss: 0.0470 - acc: 0.9852 - val_loss: 0.0373 - val_acc: 0.9878\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 316s - loss: 0.0453 - acc: 0.9851 - val_loss: 0.0431 - val_acc: 0.9856\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 326s - loss: 0.0417 - acc: 0.9871 - val_loss: 0.0383 - val_acc: 0.9879\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 325s - loss: 0.0377 - acc: 0.9885 - val_loss: 0.0355 - val_acc: 0.9888\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 314s - loss: 0.0355 - acc: 0.9888 - val_loss: 0.0374 - val_acc: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11440ff90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_BNAug_model = ConvModelBN()\n",
    "CNN_BNAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                              validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "print(\"Learning Rate, η = 0.1\")\n",
    "CNN_BNAug_model.optimizer=0.1\n",
    "CNN_BNAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=2, verbose=1,\n",
    "                              validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "print(\"Learning Rate, η = 0.01\")\n",
    "CNN_BNAug_model.optimizer=0.01\n",
    "CNN_BNAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=6, verbose=1,\n",
    "                              validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate, η = 0.1\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 316s - loss: 0.0355 - acc: 0.9884 - val_loss: 0.0362 - val_acc: 0.9887\n",
      "Learning Rate, η = 0.01\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 320s - loss: 0.0309 - acc: 0.9896 - val_loss: 0.0314 - val_acc: 0.9898\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 335s - loss: 0.0314 - acc: 0.9898 - val_loss: 0.0320 - val_acc: 0.9901\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 318s - loss: 0.0298 - acc: 0.9906 - val_loss: 0.0270 - val_acc: 0.9917\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 327s - loss: 0.0302 - acc: 0.9902 - val_loss: 0.0321 - val_acc: 0.9906\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 337s - loss: 0.0284 - acc: 0.9909 - val_loss: 0.0261 - val_acc: 0.9920\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 350s - loss: 0.0255 - acc: 0.9918 - val_loss: 0.0282 - val_acc: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10c08eb90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some more training at 0.1 and 0.01:\n",
    "print(\"Learning Rate, η = 0.1\")\n",
    "CNN_BNAug_model.optimizer=0.1\n",
    "CNN_BNAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                              validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "print(\"Learning Rate, η = 0.01\")\n",
    "CNN_BNAug_model.optimizer=0.01\n",
    "CNN_BNAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=6, verbose=1,\n",
    "                              validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Dropout + Batch Normalization + Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvModelBNDo():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1, 28, 28), output_shape=(1, 28, 28)),\n",
    "        Convolution2D(32, 3, 3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(32, 3, 3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(64, 3, 3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(64, 3, 3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 322s - loss: 0.2264 - acc: 0.9320 - val_loss: 0.0660 - val_acc: 0.9783\n",
      "Learning Rate, η = 0.1\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 364s - loss: 0.0929 - acc: 0.9711 - val_loss: 0.0707 - val_acc: 0.9775\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 343s - loss: 0.0738 - acc: 0.9771 - val_loss: 0.0459 - val_acc: 0.9856\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 327s - loss: 0.0638 - acc: 0.9804 - val_loss: 0.0439 - val_acc: 0.9870\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 410s - loss: 0.0641 - acc: 0.9806 - val_loss: 0.0600 - val_acc: 0.9816\n",
      "Learning Rate, η = 0.01\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 327s - loss: 0.0570 - acc: 0.9823 - val_loss: 0.0396 - val_acc: 0.9863\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 312s - loss: 0.0543 - acc: 0.9835 - val_loss: 0.0436 - val_acc: 0.9854\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 311s - loss: 0.0498 - acc: 0.9849 - val_loss: 0.0314 - val_acc: 0.9890\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 317s - loss: 0.0468 - acc: 0.9855 - val_loss: 0.0341 - val_acc: 0.9894\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 330s - loss: 0.0461 - acc: 0.9858 - val_loss: 0.0340 - val_acc: 0.9888\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 327s - loss: 0.0429 - acc: 0.9862 - val_loss: 0.0284 - val_acc: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1146df550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_BNDoAug_model = ConvModelBNDo()\n",
    "CNN_BNDoAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                                validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "print(\"Learning Rate, η = 0.1\")\n",
    "CNN_BNDoAug_model.optimizer.lr=0.1\n",
    "CNN_BNDoAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=4, verbose=1,\n",
    "                                validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "print(\"Learning Rate, η = 0.01\")\n",
    "CNN_BNDoAug_model.optimizer.lr=0.01\n",
    "CNN_BNDoAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=6, verbose=1,\n",
    "                                validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 326s - loss: 0.0420 - acc: 0.9873 - val_loss: 0.0322 - val_acc: 0.9910\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 394s - loss: 0.0381 - acc: 0.9883 - val_loss: 0.0284 - val_acc: 0.9904\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 355s - loss: 0.0402 - acc: 0.9871 - val_loss: 0.0325 - val_acc: 0.9905\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 477s - loss: 0.0371 - acc: 0.9883 - val_loss: 0.0230 - val_acc: 0.9926\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 383s - loss: 0.0360 - acc: 0.9891 - val_loss: 0.0325 - val_acc: 0.9897\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 441s - loss: 0.0334 - acc: 0.9891 - val_loss: 0.0251 - val_acc: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b359210>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 more epochs at 0.01\n",
    "CNN_BNDoAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=6, verbose=1,\n",
    "                                validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate η = 0.001\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 314s - loss: 0.0346 - acc: 0.9894 - val_loss: 0.0326 - val_acc: 0.9906\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 312s - loss: 0.0324 - acc: 0.9902 - val_loss: 0.0234 - val_acc: 0.9936\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 310s - loss: 0.0335 - acc: 0.9895 - val_loss: 0.0218 - val_acc: 0.9930\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 309s - loss: 0.0329 - acc: 0.9896 - val_loss: 0.0337 - val_acc: 0.9908\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 310s - loss: 0.0314 - acc: 0.9902 - val_loss: 0.0211 - val_acc: 0.9928\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 309s - loss: 0.0298 - acc: 0.9910 - val_loss: 0.0287 - val_acc: 0.9925\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 312s - loss: 0.0275 - acc: 0.9921 - val_loss: 0.0186 - val_acc: 0.9942\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 318s - loss: 0.0272 - acc: 0.9914 - val_loss: 0.0234 - val_acc: 0.9926\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 318s - loss: 0.0279 - acc: 0.9917 - val_loss: 0.0231 - val_acc: 0.9934\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 318s - loss: 0.0272 - acc: 0.9921 - val_loss: 0.0219 - val_acc: 0.9929\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 318s - loss: 0.0272 - acc: 0.9916 - val_loss: 0.0219 - val_acc: 0.9927\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 318s - loss: 0.0255 - acc: 0.9918 - val_loss: 0.0192 - val_acc: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b0d5c90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Learning Rate η = 0.001\")\n",
    "CNN_BNDoAug_model.optimizer.lr=0.001\n",
    "CNN_BNDoAug_model.fit_generator(trn_batches, trn_batches.n, nb_epoch=12, verbose=1,\n",
    "                                validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to automatically train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I'll set it to display progress at the start of each LR-change\n",
    "def train_model():\n",
    "    model = ConvModelBNDo()\n",
    "    model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                                    validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "    \n",
    "    model.optimizer.lr=0.1\n",
    "    model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                                    validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "    model.fit_generator(trn_batches, trn_batches.n, nb_epoch=3, verbose=0,\n",
    "                                    validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "    \n",
    "    model.optimizer.lr=0.01\n",
    "    model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                                    validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "    model.fit_generator(trn_batches, trn_batches.n, nb_epoch=11, verbose=0,\n",
    "                                    validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "    \n",
    "    model.optimizer.lr=0.001\n",
    "    model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                                    validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "    model.fit_generator(trn_batches, trn_batches.n, nb_epoch=11, verbose=0,\n",
    "                                    validation_data=tst_batches, nb_val_samples=tst_batches.n)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 17s - loss: 0.2290 - acc: 0.9318 - val_loss: 0.0768 - val_acc: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa41578f190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running a little test on the GPU now\n",
    "testmodel = ConvModelBNDo()\n",
    "testmodel.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                        validation_data=tst_batches, nb_val_samples=tst_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I finally got my GPU running on my workstation. Decided to leave the ghost of Bill Gates alone and put Ubuntu Linux on the second harddrive. This nvidia GTX 870M takes 17 seconds to get through the 60,000 images. The Core i5 on my Mac took an average of 340. A 20x speed up. This also means, at those numbers, a 6-strong ensemble running the regime in ```train_model()``` will take about 49 minutes and 18 seconds, instead of 16 hours and 26 minutes. You can see what the motivation was, for me to spend ~9 hours today and get the GPU working. It's a warm feeling, knowing your computer isn't just good for playing DOOM, but'll be doing its share of work real soon.\n",
    "\n",
    "So, onward:\n",
    "\n",
    "Create an array of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2289 - acc: 0.9310 - val_loss: 0.0722 - val_acc: 0.9792\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0966 - acc: 0.9700 - val_loss: 0.0529 - val_acc: 0.9835\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0584 - acc: 0.9821 - val_loss: 0.0392 - val_acc: 0.9872\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0314 - acc: 0.9904 - val_loss: 0.0271 - val_acc: 0.9904\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2352 - acc: 0.9289 - val_loss: 0.0715 - val_acc: 0.9770\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0950 - acc: 0.9706 - val_loss: 0.0708 - val_acc: 0.9777\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0587 - acc: 0.9819 - val_loss: 0.0365 - val_acc: 0.9884\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0354 - acc: 0.9892 - val_loss: 0.0293 - val_acc: 0.9900\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2202 - acc: 0.9340 - val_loss: 0.0673 - val_acc: 0.9793\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0912 - acc: 0.9723 - val_loss: 0.0453 - val_acc: 0.9855\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0569 - acc: 0.9829 - val_loss: 0.0438 - val_acc: 0.9858\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0331 - acc: 0.9897 - val_loss: 0.0247 - val_acc: 0.9923\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2289 - acc: 0.9319 - val_loss: 0.0627 - val_acc: 0.9802\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0931 - acc: 0.9710 - val_loss: 0.0497 - val_acc: 0.9838\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0577 - acc: 0.9822 - val_loss: 0.0375 - val_acc: 0.9884\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0328 - acc: 0.9899 - val_loss: 0.0216 - val_acc: 0.9931\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2161 - acc: 0.9357 - val_loss: 0.1182 - val_acc: 0.9649\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0955 - acc: 0.9708 - val_loss: 0.0447 - val_acc: 0.9861\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0560 - acc: 0.9827 - val_loss: 0.0342 - val_acc: 0.9894\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0354 - acc: 0.9896 - val_loss: 0.0314 - val_acc: 0.9902\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2269 - acc: 0.9312 - val_loss: 0.0669 - val_acc: 0.9784\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0964 - acc: 0.9696 - val_loss: 0.0493 - val_acc: 0.9841\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0563 - acc: 0.9826 - val_loss: 0.0475 - val_acc: 0.9849\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0347 - acc: 0.9891 - val_loss: 0.0224 - val_acc: 0.9933\n"
     ]
    }
   ],
   "source": [
    "# this'll take some time\n",
    "models = [train_model() for m in xrange(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the models' weights -- bc this wasn't computationally cheap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named h5py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-df3862c740f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'models/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'MNIST_CNN'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   2643\u001b[0m                     \u001b[0mstoring\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mweight\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamed\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mweight\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m         \"\"\"\n\u001b[0;32m-> 2645\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m         \u001b[0;31m# If file exists and should not be overwritten:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named h5py"
     ]
    }
   ],
   "source": [
    "from os import getcwd\n",
    "path = getcwd() + 'data/mnist/'\n",
    "model_path = path + 'models/'\n",
    "for i,m in enumerate(models):\n",
    "    m.save_weights(model_path + 'MNIST_CNN' + str(i) + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array of predictions from the models on the test-set. I'm using a batch size of ```256``` because that's what was done in lecture, and prediction is such an easier task that I think the large size just helps things go faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_preds = np.stack([m.predict(x_test, batch_size=256) for m in models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, take the average of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_preds = ensemble_preds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.996999979019165, dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.metrics.categorical_accuracy(y_test, avg_preds).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom. 0.99699.. ~ 99.7% accuracy. Same as achieved in lecture; took roughly 50 minutes to train. Unfortunately I didn't have the h5py module installed when I ran this, so the weight's can't be saved easily -- simple fix of rerunning after install."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Trying the above again, this time having h5py installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2259 - acc: 0.9318 - val_loss: 0.1109 - val_acc: 0.9646\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0922 - acc: 0.9715 - val_loss: 0.0647 - val_acc: 0.9788\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0595 - acc: 0.9818 - val_loss: 0.0338 - val_acc: 0.9887\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0334 - acc: 0.9900 - val_loss: 0.0374 - val_acc: 0.9882\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2242 - acc: 0.9315 - val_loss: 0.0697 - val_acc: 0.9773\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0945 - acc: 0.9711 - val_loss: 0.0560 - val_acc: 0.9828\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0582 - acc: 0.9825 - val_loss: 0.0391 - val_acc: 0.9875\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0342 - acc: 0.9894 - val_loss: 0.0233 - val_acc: 0.9931\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2253 - acc: 0.9323 - val_loss: 0.0667 - val_acc: 0.9796\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0928 - acc: 0.9712 - val_loss: 0.0492 - val_acc: 0.9833\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0585 - acc: 0.9819 - val_loss: 0.0395 - val_acc: 0.9869\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0324 - acc: 0.9896 - val_loss: 0.0218 - val_acc: 0.9933\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2199 - acc: 0.9336 - val_loss: 0.0652 - val_acc: 0.9784\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0918 - acc: 0.9715 - val_loss: 0.0481 - val_acc: 0.9846\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0562 - acc: 0.9826 - val_loss: 0.0373 - val_acc: 0.9876\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0335 - acc: 0.9896 - val_loss: 0.0285 - val_acc: 0.9907\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2272 - acc: 0.9316 - val_loss: 0.0839 - val_acc: 0.9744\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0898 - acc: 0.9724 - val_loss: 0.0794 - val_acc: 0.9738\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0589 - acc: 0.9823 - val_loss: 0.0527 - val_acc: 0.9838\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0348 - acc: 0.9891 - val_loss: 0.0241 - val_acc: 0.9928\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.2237 - acc: 0.9324 - val_loss: 0.0680 - val_acc: 0.9767\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0912 - acc: 0.9721 - val_loss: 0.0549 - val_acc: 0.9838\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0568 - acc: 0.9825 - val_loss: 0.0385 - val_acc: 0.9876\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s - loss: 0.0337 - acc: 0.9895 - val_loss: 0.0308 - val_acc: 0.9912\n"
     ]
    }
   ],
   "source": [
    "# this'll take some time\n",
    "models = [train_model() for m in xrange(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import getcwd\n",
    "import os\n",
    "path = getcwd() + '/data/mnist/'\n",
    "model_path = path + 'models/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir('data')\n",
    "    os.mkdir('data/mnist')\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)\n",
    "\n",
    "for i,m in enumerate(models):\n",
    "    m.save_weights(model_path + 'MNIST_CNN' + str(i) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_preds = np.stack([m.predict(x_test, batch_size=256) for m in models])\n",
    "avg_preds = ensemble_preds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.9970999956130981, dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.metrics.categorical_accuracy(y_test, avg_preds).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. 99.71% -- 19 May 2017 - Wayne H Nixalo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
