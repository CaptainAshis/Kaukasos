{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a classifier for the Jigsaw [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) hosted by Kaggle, written in Keras/TF. This model leverages multiple insights from high-scoring Kaggle kernels, as well as exploratory work in PyTorch's Torchtext library.\n",
    "\n",
    "In this notebook I'll compare a simply bidirectional LSTM with a more complex bi-GRU-ConvNet. If the two models produce similar predictions, I'll take the simpler model if it isn't dramatically worse, and ensemble it with a NB-SVM.\n",
    "\n",
    "**See Aside 1** for terms.\n",
    "\n",
    "-- Wayne Nixalo 1/4/2018\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data & Embeddings Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.preprocessing.text\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = pathlib.Path('../../data')\n",
    "PATH_COMP = PATH_DATA/'competitions/jigsaw-toxic-comment-classification-challenge'\n",
    "TRAIN_FILE = 'train.csv'\n",
    "TEST_FILE  = 'test.csv'\n",
    "EMBEDDING_GLOVE    = PATH_DATA/'glove/glove.6B.50d.txt'\n",
    "EMBEDDING_FASTTEXT = PATH_DATA/'fasttext/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Config Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_sz = 50    # embedding vector length\n",
    "max_feat = 20000 # num unique words\n",
    "maxlen   = 100   # max length of sequence to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing **See: Aside 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data & replace missing values\n",
    "train_df = pd.read_csv(PATH_COMP/TRAIN_FILE)\n",
    "test_df  = pd.read_csv(PATH_COMP/TEST_FILE)\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_sentences_test  = test_df[\"comment_text\"].fillna(\"_na_\").values\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "labels = train_df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, numericalize, and pad\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_feat, lower=True, oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test  = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "input_train = keras.preprocessing.sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "input_test  = keras.preprocessing.sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained word vectors (GloVe or fastText) into word->vector dictionary **See: Aside 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word-vector lookup dictionary\n",
    "def get_coefficients(word, *arr):\n",
    "    \"\"\"return a word and an ndarray of its associated vector embedding\"\"\"\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefficients(*o.strip().split()) for o in open(EMBEDDING_GLOVE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use embedding size of the pretrained vectors:\n",
    "# embed_sz = len(embeddings_index[next(iter(embeddings_index.keys()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Embedding Matrix. Randomly initialize out-of-vocab words to the mean & standard deviation of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.stack(embeddings_index.values())\n",
    "emb_mean   = embeddings.mean()\n",
    "emb_stdv   = embeddings.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words    = min(max_feat, tokenizer.num_words)\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_stdv, (n_words, embed_sz))\n",
    "\n",
    "# build using the first `max_feat` most common words\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= max_feat:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM():\n",
    "    m_input = keras.layers.Input(shape=(maxlen,))\n",
    "    x = keras.layers.Embedding(max_feat, embed_sz, weights=[embedding_matrix])(m_input)\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(50, return_sequences=True, \n",
    "                                                     dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    x = keras.layers.GlobalMaxPool1D()(x)\n",
    "    x = keras.layers.Dense(50, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dropout(0.1)(x)\n",
    "    x = keras.layers.Dense(len(list_classes), activation=\"sigmoid\")(x)\n",
    "    \n",
    "    return keras.models.Model(inputs=m_input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiGRU_ConvNet():\n",
    "    m_input = keras.layers.Input(shape=(maxlen,))\n",
    "    x = keras.layers.Embedding(max_feat, embed_sz, \n",
    "                               weights=[embedding_matrix], \n",
    "                               trainable=False)(m_input)\n",
    "    x = keras.layers.SpatialDropout1D(0.2)(x)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(128, \n",
    "                                                    return_sequences=True,\n",
    "                                                    dropout=0.1, \n",
    "                                                    recurrent_dropout=0.1))(x)\n",
    "    x = keras.layers.Conv1D(64, kernel_size=3)(x)\n",
    "    x = keras.layers.concatenate([\n",
    "            keras.layers.GlobalAveragePooling1D()(x),\n",
    "            keras.layers.GlobalMaxPooling1D()(x)\n",
    "        ])\n",
    "    x = keras.layers.Dense(len(list_classes), activation=\"sigmoid\")(x)\n",
    "    \n",
    "    return keras.models.Model(inputs=m_input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM()\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(input_train, labels, batch_size=64, epochs=2, validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiGRU_ConvNet()\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does not demonstrate Cross Validation. The built-in Keras `validation_split` parameter in `Model.fit` sets aside a fraction of the data *before* it is shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside 1: Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM: [Long Short-Term Memory Network](http://colah.github.io/posts/2015-08-Understanding-LSTMs/#lstm-networks)\n",
    "- GRU : [Gated Recurrent Unit Network](http://colah.github.io/posts/2015-08-Understanding-LSTMs/#variants-on-long-short-term-memory)\n",
    "- NB-SVM: [Naïve Bayes Support Vector Machine](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)\n",
    "- ConvNet: [Convolutional Neural Network](http://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Aside 2: Sequence Padding: Keras vs. PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "PyTorch's `torchtext.data.BucketIterator` pads by batch, whereas Keras' `keras.preprocessing.text.pad_sequences` pads the entire dataset. \n",
    "\n",
    "This points to some memory savings in PyTorch and makes it some to pay attention to as the library and as FastAI integration develop. ie: Keras pads all sequences in the dataset to `maxlen`, but PyTorch sorts each batch by length and pads to the longest-length sequence in each batch. This seems like something doable in TensorFlow that's hidden by Keras' level of abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Aside 3: Taking a look at embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GloVe 50d - First Vector\n",
    "for o in open(EMBEDDING_GLOVE):\n",
    "    print(len(o.split()) - 1)\n",
    "    print(o)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      "\n",
      "the 0.0231 0.0170 0.0157 -0.0773 0.1088 0.0031 -0.1487 -0.2672 -0.0357 -0.0487 0.0807 0.1532 -0.0739 -0.0291 -0.0445 -0.0014 0.1014 0.0186 -0.0253 0.0200 -0.0026 -0.0179 0.0005 0.0054 -0.0134 0.0233 -0.0755 -0.0156 0.0415 -0.4985 0.0410 -0.0616 0.0047 0.0325 -0.0162 -0.0172 0.0988 0.0766 -0.0796 -0.0345 0.0124 -0.1007 -0.0292 -0.0762 -0.1261 -0.0531 0.0424 0.0144 -0.0683 0.2859 0.0399 0.0201 0.3240 -0.0656 -0.0497 0.0090 0.0902 -0.0138 -0.0412 -0.0297 0.3139 -0.1428 0.0166 -0.0219 -0.0575 0.1359 -0.1655 0.0019 0.0323 -0.0013 -0.3033 -0.0091 0.1462 0.1860 -0.0524 0.1886 -0.7372 -0.0248 -0.0205 0.0022 0.5988 -0.0359 -0.0269 -0.0483 0.0109 -0.0044 0.0592 0.0174 0.0010 -0.0012 -0.0251 0.4620 -0.0443 -0.0350 0.0115 0.1496 0.3125 -0.0091 0.2517 0.0654 0.0237 -0.0432 0.0952 0.0650 -0.2932 0.0630 0.0236 0.0340 -0.0012 0.0889 -0.0006 -0.1736 0.0374 0.0313 -0.6184 0.0282 -0.3836 0.0589 0.2443 0.0602 0.0057 -0.0038 0.1352 0.0053 0.0193 -0.0213 0.0248 0.0214 0.2334 -0.0438 0.0527 0.0262 0.0655 -0.0859 0.2642 -0.0393 -0.0163 0.0681 -0.0175 -0.1158 0.0950 0.0475 0.0069 0.5164 -0.0026 -0.0255 -0.0801 -0.0262 0.1113 0.0798 -0.0015 0.0252 -0.0379 -0.0260 -0.0282 -0.0420 0.0482 -0.0175 0.0282 0.0400 0.3998 -0.1054 0.0755 0.1027 -0.0199 0.0381 -0.0333 -0.0342 0.0267 0.0865 0.0024 -0.0091 0.0163 -0.0287 0.0364 -0.0202 -0.0367 -0.0356 -0.0614 -0.0551 0.2649 -0.0371 0.0207 0.0364 0.0512 -0.0843 -0.0138 0.0710 0.0843 0.0291 -0.0100 0.0398 -0.0646 -0.0595 -0.0258 -0.0282 0.0311 0.0147 -0.0449 0.0276 -0.1168 0.0219 -0.0231 -0.0162 -0.0286 0.0128 -0.0259 0.0153 0.1042 -0.1207 -0.0135 0.5405 -0.0362 0.0476 -0.0180 -0.0735 0.0034 -0.0026 -0.0057 0.0380 -0.0401 -0.1016 0.0344 0.0402 0.0513 -0.0815 0.0390 0.0076 0.0175 0.0030 -0.0707 0.0150 -0.1174 0.0266 -0.0795 0.1988 0.0978 -0.0587 -0.0533 0.0273 0.0442 -0.0463 -0.0708 0.0176 -0.0994 0.0846 0.3620 -0.0207 0.0256 -0.0145 0.0309 0.0082 0.0042 -0.0314 0.1196 -0.0346 0.0386 -0.0368 -0.0333 -0.0032 -0.0048 -0.0006 0.0509 -0.0232 0.1183 -0.1314 0.0149 0.0762 -0.0161 0.0160 0.0390 -0.1922 0.0031 -0.0666 0.0593 -0.0621 0.0421 0.0328 -0.0901 -0.0159 0.1015 0.6164 -0.0650 0.1241 0.0059 0.0653 -0.0386 0.0166 0.0403 0.0169 -0.0008 0.0052 -0.0363 -0.2508 0.1252 -0.1008 -0.0308 0.0744 -0.1118 0.0963 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fastText 300d - First Vector\n",
    "i = 0\n",
    "for o in open(EMBEDDING_FASTTEXT):\n",
    "    if i == 0:\n",
    "        print(o)\n",
    "    if i == 2: \n",
    "        print(o)\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Aersu)",
   "language": "python",
   "name": "aersu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
