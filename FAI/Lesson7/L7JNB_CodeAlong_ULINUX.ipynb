{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wayne H Nixalo - 5 July 2017 : 00:02\n",
    "\n",
    "FAI01 - Practical Deep Learning I\n",
    "\n",
    "Lesson 7: Exotic CNN Architectures - JNB CodeAlong\n",
    "\n",
    "[L7JNB](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson7.ipynb) | [Lecture 7](https://youtu.be/Q0z-l2KRYFY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisheries competition\n",
    "\n",
    "In this notebook we're going to investigate a range of difference architectures for the [Kaggle Fisheries competition](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/). The video states that `vgg.py` and `vgg_ft()` from `utils.py` have been updated to include VGG with Batch Normalization, but this isn't the case. We've instead created a new file [vgg_bn.py](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/vgg16bn.py) and an additional method `vgg_ft_bn()` (which is already in `utils.py`) which we use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 870M (0000:01:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.join('../utils'))\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"data/fisheries/sample/\" # MACOSX\n",
    "path = \"data/fisheries/\" # UBUNTULINUX\n",
    "# batch_size = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wnixalo/Kaukasos/FAI\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 13153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "(val_classes, trn_classes, val_labels, trn_labels,\n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someties it's helpful to have just the filenames, without the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_filenames      = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames  = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## I. Setup Dirs:\n",
    "\n",
    "We create the validation and sample sets in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd data/fisheries\n",
    "%cd train\n",
    "%mkdir ../valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd data/fisheries/test_stg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = glob('*.jpg')\n",
    "for img in g: os.rename(img, '../test/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = glob('*.jpg')\n",
    "for img in g: os.rename(img, 'unknown/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# makes a copy of each category folder in /valid\n",
    "g = glob('*')\n",
    "for d in g: os.mkdir('../valid/' + d)\n",
    "\n",
    "# move 500 random folder/img pairs from /train to /valid\n",
    "# not perfectly proportional (closer w/ more imgs)\n",
    "g = glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(500): os.rename(shuf[i], '../valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir ../sample\n",
    "%mkdir ../sample/train\n",
    "%mkdir ../sample/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile # copying files requires module import\n",
    "\n",
    "# make corresponding categ folders in sample dir\n",
    "g = glob('*')\n",
    "for d in g:\n",
    "    os.mkdir('../sample/train/' + d)\n",
    "    os.mkdir('../sample/valid/' + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy 400 random folder/img pairs to sample folder (val:200)\n",
    "\n",
    "# for training set\n",
    "g = glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(400): copyfile(shuf[i], '../sample/train/' + shuf[i])\n",
    "\n",
    "%cd ../valid\n",
    "\n",
    "# for valid set\n",
    "g = glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(200): copyfile(shuf[i], '../sample/valid/' + shuf[i])\n",
    "    \n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir results\n",
    "%mkdir sample/results\n",
    "%cd ../..\n",
    "# %cd data/fisheries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## II. Basic VGG\n",
    "\n",
    "We start with our usual VGG approach. We'll be using VGG w/ Batch Normalization. We explained how to add batch normalization to VGG in the [imagenet_batchnorm_notebook](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/imagenet_batchnorm.ipynb). VGG with batchnorm is implemented in [vgg_bn.py](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/vgg16bn.py), and there is a version of `vgg_ft` (our finetuning function) with batch norm called `vgg_ft_bn` in [utils.py](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/utils.py).\n",
    "\n",
    "### Initial Model\n",
    "\n",
    "First we create a simple fine-tuned VGG model to be our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vgg16bn import Vgg16BN\n",
    "model = vgg_ft_bn(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_data(.)` calls `get_batches(.)` and uses `np.concatenate(.)` to create one long array of all the data. Yikes RIP Memory. `get_batches(.)` calls `gen.flow_from_directory(.)`. This is good and friendly: I can simply call `get_batches(.)` directory, as it returns a batch generator. This solveds the memory issue. On a Linux machine w/ 16 GB of RAM, loading `trn`, `val`, and `test` directly into memory via `get_data(.)` uses up 99.5% of Memory and 38.5% of Swap. That is very not good. I haven't even done any convolutional stuff yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(path + 'train')\n",
    "val = get_data(path + 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test = get_data(path + 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(path + 'results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path + 'results/trn.dat', trn)\n",
    "save_array(path + 'results/val.dat', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path + 'results/test.dat', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = load_array(path + 'results/trn.dat')\n",
    "val = load_array(path + 'results/val.dat')\n",
    "test = load_array(path + 'results/test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(1e-3),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trn, trn_labels, batch_size=batch_size, nb_epoch=3, validation_data=(val, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path + 'results/ft1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute Convolutional Output\n",
    "\n",
    "We pre-compute the output of the last convolutional layer of VGG, since we're unlikely to need to fine-tune those layers. (All following analysis will be done on just the pre-computed convolutional features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights(path + 'results/ft1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just took a look at `spli_at(..)` in utils.py; this function's awesome\n",
    "conv_layers, fc_layers = split_at(model, Convolution2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_feat = conv_model.predict(trn)\n",
    "conv_val_feat = conv_model.predict(val)\n",
    "# conv_test_feat = conv_model.predict(test)\n",
    "\n",
    "# NOTE: looking at MACOSX mem usage, I think Apple does some memory-caching black magic \n",
    "# to not hit the machine's memory cap. <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path + 'results/conv_feat.dat', conv_feat)\n",
    "save_array(path + 'results/conv_val_feat.dat', conv_val_feat)\n",
    "# save_array(paht + 'results/conv_test_feat.dat', conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_feat = load_array(path + 'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path + 'results/conv_val_feat.dat')\n",
    "# conv_test_feat = load_array(path + 'results/conv_test_feat.dat', conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "We can now create our first baseline model - a simple 3-Layer FC Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        BatchNormalization(axis=1),\n",
    "        Dropout(p/4),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(8, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=3,\n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr = 1e-4\n",
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=7,\n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path + 'models/conv_512_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.evaluate(conv_val_feat, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.load_weights(path + 'models/conv_512_6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## III. Multi-Input\n",
    "\n",
    "The images are of different sizes, which are likely to represent the boat they came from (since different boats will use different cameras). Perhaps this creates some data leakage that we can take advantage of to get a better Kaggle leaderboard position? To find out, first we createa arrays of the file sizes for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates array of all image sizes\n",
    "sizes = [PIL.Image.open(path + 'train/' + f).size for f in filenames]\n",
    "# creates set of image sizes (all unique sizes)\n",
    "id2size = list(set(sizes))\n",
    "# creates dictionary of image size to index <-- for one-hotting\n",
    "size2id = {o:i for i,o in enumerate(id2size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "collections.Counter(sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we One-Hot Encode them (since we want to treat them as categorical) and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encode images\n",
    "trn_sizes_orig = to_categorical([size2id[o] for o in sizes], len(id2size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sizes_orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same proc as `sizes` above; for val imgs\n",
    "raw_val_sizes = [PIL.Image.open(path + 'valid/' + f).size for f in val_filenames]\n",
    "id2size = list(set(raw_val_sizes))\n",
    "size2id = {o:i for i,o in enumerate(id2size)}\n",
    "\n",
    "# one-hot encoding\n",
    "val_sizes = to_categorical([size2id[o] for o in raw_val_sizes], len(id2size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aha, this get's the normalized sizes of the images:\n",
    "# subtract mean, divide by standard deviation\n",
    "trn_sizes = trn_sizes_orig - trn_sizes_orig.mean(axis=0) / trn_sizes_orig.std(axis=0)\n",
    "val_sizes = val_sizes - trn_sizes_orig.mean(axis=0) / trn_sizes_orig.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2size[:]\n",
    "size2id\n",
    "# sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this additional 'meta-data', we create a model with multiple input layers - `sz_inp` will be our input for the size information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: convolutional layers\n",
    "inp = Input(conv_layers[-1].output_shape[1:])\n",
    "# size input\n",
    "sz_inp = Input((len(id2size),))\n",
    "# batchnormed size input\n",
    "bn_inp = BatchNormalization()(sz_inp)\n",
    "\n",
    "# as usual: convolutional features in:\n",
    "x = MaxPooling2D()(inp)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = Dropout(p/4)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p/2)(x)\n",
    "# size input merged in here\n",
    "x = merge([x, bn_inp], 'concat')\n",
    "x = Dense(8, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compile the mdoel, we have to specify all the input layers in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model([inp, sz_inp], x)\n",
    "model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we train the model, we have to provide all the input layers' data in an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([conv_feat, trn_sizes], trn_labels, batch_size=batch_size, nb_epoch=3,\n",
    "             validation_data=([conv_val_feat, val_sizes], val_labels))\n",
    "# NOTE: this is attempting to take advantage of data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this is comparing multi input model to single input model:\n",
    "bn_model.optimizer.lr = 1e-4\n",
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=8,\n",
    "          validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: this Multi-IO model only outperforms standard BN model in the first few epochs (see first runs of BN Model)*\n",
    "\n",
    "The model didn't show an improvement by using the leakage, other than in the early epochs. THis is most likely because the information about what boat the picture came from is readily identified from the image itself, so the meta-data turned out not to add any additional information.\n",
    "\n",
    "---\n",
    "## IV. Bounding Boxes & Multi Output\n",
    "\n",
    "### Import / View Bounding Boxes\n",
    "\n",
    "A kaggle user has created bounding box annotations for each fish in each training set image. You can download them [from here](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/forums/t/25902/complete-bounding-box-annotation). We'll see if we can utilize this additional information. First, we'll load in the data, and keep just the largest bounding box for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UltraJSON -- \"ultra-fast JSON decoder for Python, written in pure C\"\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_classes = ['alb', 'bet', 'dol', 'lag', 'other', 'shark', 'yft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb_json = {}\n",
    "for c in anno_classes:\n",
    "    j = json.load(open('{}annos/{}_labels.json'.format(path, c), 'r'))\n",
    "    for l in j:\n",
    "        if 'annotations' in l.keys() and len(l['annotations']) > 0:\n",
    "            bb_json[l['filename'].split('/')[-1]] = sorted(\n",
    "                l['annotations'], key=lambda x: x['height'] * x['width'])[-1]\n",
    "# format of j:\n",
    "# [\n",
    "#     {\n",
    "#         \"annotations\": [\n",
    "#             {\n",
    "#                 \"class\": \"rect\",\n",
    "#                 \"height\": 89.0,\n",
    "#                 \"width\": 355.0,\n",
    "#                 \"x\": 512.0,\n",
    "#                 \"y\": 162.0\n",
    "#             },\n",
    "#             .......\n",
    "#             .......\n",
    "#         ],\n",
    "#         \"class\": \"image\",\n",
    "#         \"filename\": \"img_07825.jpg\"\n",
    "#     },\n",
    "#     .....\n",
    "#     .....\n",
    "# ]\n",
    "\n",
    "# the files need to be downloaded and placed in `path`/annos/ first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_json['img_04908.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file2idx = {o:i for i,o in enumerate(raw_filenames)}\n",
    "val_file2idx = {o:i for i,o in enumerate(raw_val_filenames)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any images that have no annotations, we'll create an empt bounding box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_bbox = {'height': 0., 'width': 0., 'x': 0., 'y': 0.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in raw_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox\n",
    "for f in raw_val_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert the dictionary into an array, and convert the coordiantes to our resized 224x224 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb_params = ['height', 'width', 'x', 'y']\n",
    "def convert_bb(bb, size):\n",
    "    bb = [bb[p] for p in bb_params]\n",
    "    conv_x = (224. / size[0])\n",
    "    conv_y = (224. / size[1])\n",
    "    bb[0] = bb[0] * conv_y\n",
    "    bb[1] = bb[1] * conv_x\n",
    "    bb[2] = max(bb[2] * conv_x, 0)\n",
    "    bb[3] = max(bb[3] * conv_y, 0)\n",
    "    return bb\n",
    "# fn creates a list of the largest fish in ea. image & places box arn them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_bbox = np.stack([convert_bb(bb_json[f], s) for f,s in zip(raw_filenames, sizes)],\n",
    "                   ).astype(np.float32)\n",
    "val_bbox = np.stack([convert_bb(bb_json[f], s)\n",
    "                    for f,s in zip(raw_val_filenames, raw_val_sizes)]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check our work by drawing one of the annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_rect(bb, color='red'):\n",
    "    return plt.Rectangle((bb[2], bb[3]), bb[1], bb[0], color=color, fill=False, lw=3)\n",
    "def show_bb(i):\n",
    "    bb = val_bbox[i]\n",
    "    plot(val[i])\n",
    "    plt.gca().add_patch(create_rect(bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bb(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & Train Model:\n",
    "\n",
    "Sicne we're not allowed (by the Kaggle rules) to manually annotate the test set, we'll need to create a model that predicts the locations of the bounding box on each image. To do so, we create a model with multiple outputs: it'll predict both the type of fish (the 'class'), and the 4 bounding box coordinates. We prefer this approach to only predicting the bounding box coordinates, since we hope that giving the model more context about what it's looking for will help it with both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(conv_layers[-1].output_shape[1:])\n",
    "x = MaxPooling2D()(inp)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = Dropout(p/4)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p/2)(x)\n",
    "x_bb = Dense(4, name='bb')(x) # 4 linear outps w/ no actvtn fn -- bb dims\n",
    "x_class = Dense(8, activation='softmax', name='class')(x)\n",
    "# this model'll have to figure out how to come up w/ a bunch of dense layers \n",
    "# capable of doing the last 2 lines simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have multiple outputs, we need to provide them to the model constructor in an array, and we also need to say what loss function to use for each. We also weight the bounding box loss function down by 1000x, since the scales of the Cross-Entropy Loss and the MSE are very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model([inp], [x_bb, x_class]) # optmz: x_bb:mse; x_class:catgXentrop\n",
    "model.compile(Adam(lr=0.001), loss=['mse', 'categorical_crossentropy'], metrics=['accuracy'],\n",
    "             loss_weights=[0.001, 1.0]) # scale 1st loss (mse) by 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(conv_feat, [trn_bbox, trn_labels], batch_size=batch_size, nb_epoch=3,\n",
    "             validation_data=(conv_val_feat, [val_bbox, val_labels]))\n",
    "# [trn_bbox, trn_labels], <<- use bbs as labels for 1st outp, fish types for 2nd outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit(conv_feat, [trn_bbox, trn_labels], batch_size=batch_size, nb_epoch=10,\n",
    "             validation_data=(conv_val_feat, [val_bbox, val_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: bb loss is ~1000x larger than class loss. Hense the scaling. Note also its more stable*\n",
    "\n",
    "Excitingly, it turned out that the classification model is much improved by giving it this additional task. Let's see how well the bounding box model did by taking a look at its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do prediction for 1st 10 validation images\n",
    "pred = model.predict(conv_val_feat[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_bb_pred(i):\n",
    "    bb = val_bbox[i]\n",
    "    bb_pred = pred[0][i]\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plot(val[i])\n",
    "    ax=plt.gca()\n",
    "    ax.add_patch(create_rect(bb_pred, 'yellow'))\n",
    "    ax.add_patch(create_rect(bb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image shows that it can find fish that are tricky even for us to see!\n",
    "\n",
    "*although w/ the much smaller sample dataset on the MAC, performance is pretty abysmal on the examples.. but should be v.good on the Linux machine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_bb_pred(6)\n",
    "# for i in range(40, 50):\n",
    "#     show_bb_pred(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: a powerful model will crop out these fish and run them through a second model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(conv_val_feat, [val_bbox, val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path + 'models/bn_anno.h5')\n",
    "# model.load_weights(path + 'models/bn_anno.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## V. Larger Size\n",
    "\n",
    "### Set up Data\n",
    "\n",
    "Let's see if we get better results if we use larger images. We'll use 640x360, since it's the same shape as the most common size we saw earlier (1280x720), without being too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = get_data(path + 'train', (360, 640))\n",
    "val = get_data(path + 'valid', (360, 640))\n",
    "# remember image dimensions are width x height, but \n",
    "# matrix dims are rows X columns; (reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image shows that things are much clearer at this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test = get_data(path + 'test', (360, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path + 'results/trn_640.dat', trn)\n",
    "save_array(path + 'results/val_640.dat', val)\n",
    "# save_array(path + 'results/test_640.dat', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trn = load_array(path + 'results/trn_640.dat')\n",
    "# val = load_array(path + 'results/val_640.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our VGG model - we'll need to tell it we're not using the normal 224x224 images, which also means it won't include the Fully-Connected layers (since they don't make sense for non-default sizes). We'll also remove the last Max-Pooling layer, since we don't want to throw away information yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg640 = Vgg16BN((360, 640)).model\n",
    "vgg640.pop() # we don't want the last MaxPooling layer \n",
    "# vgg640.input_shape, vgg640.output_shape\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "vgg640.input_shape, vgg640.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_val_feat = vgg640.predict(val, batch_size=32, verbose=1)\n",
    "conv_trn_feat = vgg640.predict(trn, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path + 'results/conv_val_640.dat', conv_val_feat)\n",
    "save_array(path + 'results/conv_trn_640.dat', conv_trn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_test_feat = vgg640.predict(test, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_val_feat = load_array(path + 'results/conv_val_640.dat')\n",
    "# conv_trn_feat = load_array(path + 'results/conv_trn_640.dat')\n",
    "# conv_test_feat = load_array(path + 'results/conv_test_640.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## VI. Fully Convolutional Net (FCN)\n",
    "\n",
    "Since we're using a larger input, the output of the final convolutional layer is also larger. So we probably don't want to put a dense layer there - that would be a *lot* of parameters! Instead, let's use a fully convolutional net (FCN); this also has the benefit that they tend to generalize well, and also seems like a good fit for our problem (since the fish are a small part of the image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layers, _ = split_at(vgg640, Convolution2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J.H.: I'm not using any doprout, since I found I got better results without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf = 128; p = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf, 3, 3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf, 3, 3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf, 3, 3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1, 2)), # (1,2) MxP to trsfm into square result\n",
    "        Convolution2D(8, 3, 3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_model = Sequential(get_lrg_layers())\n",
    "lrg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=2,\n",
    "              validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_model.optimizer.lr=1e-5\n",
    "lrg_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=6,\n",
    "              validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JH: When I submitted the results of this model to Kaggle, I got the best single model results of any shown here (ranked 22nd on the leaderboard as at Dec-6-2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.save_weights(path + 'models/lrg_nmp.h5')\n",
    "# lrg_model.load_weights(path + 'models/lrg_nmp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_model.evaluate(conv_val_feat, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another benefit of this kind of model is that the last convolutional layer has to learn to classify each part of the image (since there's only an Average Pooling layer after). Let's create a function that grabs the output of this layer (which is the 4th-last layer of our model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = lrg_model.layers\n",
    "conv_fn = K.function([L[0].input, K.learning_phase()], L[-4].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cm(inp, label):\n",
    "    conv = conv_fn([inp, 0])[0, label]\n",
    "    return scipy.misc.imresize(conv, (360, 640), interp='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to add an extra dimension to our input since the CNN expects a 'batch' (even if it's just a batch of one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.expand_dims(conv_val_feat[0],0)\n",
    "np.round(lrg_model.predict(inp)[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(to_plot(val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = get_cm(inp, 0)\n",
    "plt.imshow(cm, cmap=\"cool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Convolutional Net Heatmap\n",
    "\n",
    "To create a higher resolution heatmap, we'll remove all the max pooling layers, and repeat the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf, 3, 3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(nf, 3, 3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(nf, 3, 3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(8, 3, 3, border_mode='same'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_model = Sequential(get_lrg_layers())\n",
    "lrg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=2,\n",
    "              validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_model.optimizer.lr=1e-5\n",
    "lrg_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=6,\n",
    "              validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.save_weights(path + 'models/lrg_0mp.h5')\n",
    "# lrg_model.load_weights(path + 'models/lrg_0mp.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = lrg_model.layers\n",
    "conv_fn = K.function([L[0].input, K.learning_phase()], L[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resize heatmap to 360 by 640\n",
    "def get_cm2(imp, label):\n",
    "    conv = conv_fn([inp, 0])[0, label]\n",
    "    return scipy.misc.imresize(conv, (360, 640))\n",
    "# by default, .imresize will try to interpolate; replace big pixels w/ \n",
    "# interpolated small pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = np.expand_dims(conv_val_feat[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(to_plot(val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = get_cm2(inp, 0)\n",
    "# cm = get_cm2(inp, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cm, cmap=\"cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot(val[0])\n",
    "plt.imshow(cm, cmap=\"cool\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for \"No Fish\" class:\n",
    "cm = get_cm2(inp, 4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot(val[0])\n",
    "plt.imshow(cm, cmap=\"cool\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A hand is not a fish, sir. Mmmm Fish Fingers...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## VI. Inception Mini-Net\n",
    "\n",
    "Here's an example of how to create and use \"Inception Blocks\" - as you see, they use multiple different convolution filter sizes and concatenate the reusults together. We'll talk more about these next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: unable to get this working so far. Searched for a similar problem: *\n",
    "\n",
    "    ValueError: padding must be zero for average_exc_pad \n",
    "\n",
    "*and the [author of Keras said](https://github.com/fchollet/keras/issues/4599#issuecomment-296862194) the solution was to use the TensorFlow backend bc it's a Theano issue. The 2nd part of this course apparently uses InceptionV3 within Keras, so I may hold off for a better supported way like that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_bn(x, nb_filter, nb_row, nb_col, subsample=(1,1)):\n",
    "    x = Convolution2D(nb_filter, nb_row, nb_col,\n",
    "                      subsample=subsample, activation='relu', border_mode='same')(x)\n",
    "    return BatchNormalization(axis=1)(x)\n",
    "\n",
    "def incep_block(x):\n",
    "    branch1x1 = conv2d_bn(x, 32, 1, 1, subsample=(2, 2))\n",
    "    branch5x5 = conv2d_bn(x, 24, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 32, 5, 5, subsample=(2, 2))\n",
    "    \n",
    "    branch3x3dbl = conv2d_bn(x, 32, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 48, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 48, 3, 3, subsample=(2, 2))\n",
    "    \n",
    "    branch_pool = AveragePooling2D(\n",
    "        (3, 3), strides=(2, 2), border_mode='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 16, 1, 1)\n",
    "    return merge([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "                mode='concat', concat_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(vgg640.layers[-1].output_shape[1:])\n",
    "x = BatchNormalization(axis=1)(inp)\n",
    "x = incep_block(x)\n",
    "x = incep_block(x)\n",
    "x = incep_block(x)\n",
    "x = Dropout(0.75)(x)\n",
    "x = Convolution2D(8,3,3, border_mode='same')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "outp = Activation('softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_model = Model([inp], outp)\n",
    "lrg_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lrg_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=2,\n",
    "              validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copy-Paste from Lesson 7 JNB gives same result*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv2d_bn(x, nb_filter, nb_row, nb_col, subsample=(1, 1)):\n",
    "#     x = Convolution2D(nb_filter, nb_row, nb_col,\n",
    "#                       subsample=subsample, activation='relu', border_mode='same')(x)\n",
    "#     return BatchNormalization(axis=1)(x)\n",
    "\n",
    "# def incep_block(x):\n",
    "#     branch1x1 = conv2d_bn(x, 32, 1, 1, subsample=(2, 2))\n",
    "#     branch5x5 = conv2d_bn(x, 24, 1, 1)\n",
    "#     branch5x5 = conv2d_bn(branch5x5, 32, 5, 5, subsample=(2, 2))\n",
    "\n",
    "#     branch3x3dbl = conv2d_bn(x, 32, 1, 1)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 48, 3, 3)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 48, 3, 3, subsample=(2, 2))\n",
    "\n",
    "#     branch_pool = AveragePooling2D(\n",
    "#         (3, 3), strides=(2, 2), border_mode='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 16, 1, 1)\n",
    "#     return merge([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "#               mode='concat', concat_axis=1)\n",
    "\n",
    "# inp = Input(vgg640.layers[-1].output_shape[1:]) \n",
    "# x = BatchNormalization(axis=1)(inp)\n",
    "# x = incep_block(x)\n",
    "# x = incep_block(x)\n",
    "# x = incep_block(x)\n",
    "# x = Dropout(0.75)(x)\n",
    "# x = Convolution2D(8,3,3, border_mode='same')(x)\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# outp = Activation('softmax')(x)\n",
    "\n",
    "# lrg_model = Model([inp], outp)\n",
    "\n",
    "# lrg_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# lrg_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "#              validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.optimizer.lr=1e-5\n",
    "lrg_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=6,\n",
    "              validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=10,\n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.save_weights(path + 'm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "All following code not run:\n",
    "\n",
    "## VII. Pseudo-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict([conv_test_feat, test_sizes], batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batches = gen.flow(conv_test_feat, preds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_batches = gen.flow(conv_val_feat, val_labels, batch_Size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = gen.flow(conv_feat, trn_labels, batch_size=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mi = MixIterator([batches, test_batches, val_batches]) # I think that's how it's done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.fit_generator(mi, mi.N, nb_epoch=8, validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## VIII. Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1 - mx) / 7, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.evaluate(conv_val_feat, val_labels, batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(conv_test_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = load_array(path + 'results/test_640.dat')\n",
    "# test = load_array(path + 'results/test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = conv_model.predict(test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds, 0.82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm_name = path + 'results/subm_bb.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
