{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAI1 Practical Deep Learning I | 11 May 2017 | Wayne Nixalo\n",
    "\n",
    "In this notebook I'll be building a simple linear model in Keras using ```Sequential()```\n",
    "\n",
    "Tutorial on Linear Model for MNIST: [linky](https://www.kaggle.com/fchollet/simple-deep-mlp-with-keras/code/code)\n",
    "\n",
    "Keras.io doc on [.fit_generator & Sequential](https://keras.io/models/sequential/#fit_generator)\n",
    "\n",
    "##### Some Notes:\n",
    "It looks like I'll need to use Pandas to work with data in .csv files (MNIST from Kaggle comes that way). For the data sets that come in as folders of .jpegs, I'll use the way shown in class of get_batches & get_data ... but then if that's for a DLNN wouldn't I have to do that for this as well? Will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Import relevant libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data functions ~ mostly from utils.py or vgg16.py\n",
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical',\n",
    "                 target_size=(224,224)):\n",
    "    return gen.flow_from_directory(dirname, target_size=target_size,\n",
    "            class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "# def onehot(x): return to_categorical(labels, num_classes=10)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "def onehot(x): return to_categorical(x)\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n",
    "    \n",
    "# import bcolz\n",
    "# def save_data(fname, array): c=bcolz.carray(array, rootdir=fname, mode='w'); c.flush()\n",
    "# def load_data(path): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some setup\n",
    "path = 'L2HW_data/'\n",
    "if not os.path.exists(path): os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting Data\n",
    "\n",
    "# val_batches = get_batches(path+'valid/', shuffle=False, batch_size=1)\n",
    "# trn_batches = get_batches(path+'train/', shuffle=False, batch_size=1)\n",
    "\n",
    "# converting classes to OneHot for Keras\n",
    "# val_classes = val_batches.classes\n",
    "# trn_classes = trn_batches.classes\n",
    "# val_labels = onehot(val_classes)\n",
    "# trn_labels = onehot(trn_classes)\n",
    "\n",
    "# See: https://www.kaggle.com/fchollet/simple-deep-mlp-with-keras/code/code\n",
    "# for help loading\n",
    "# I haven't learned how to batch-load .csv files; I'll blow that bridge \n",
    "#     when I get to it.\n",
    "\n",
    "# read data\n",
    "import pandas as pd\n",
    "trn_data = pd.read_csv(path + 'train.csv')\n",
    "trn_labels = trn_data.ix[:,0].values.astype('int32')\n",
    "trn_input = (trn_data.ix[:,1:].values).astype('float32')\n",
    "test_input = (pd.read_csv(path + 'test.csv').values).astype('float32')\n",
    "\n",
    "# one-hot encode labels\n",
    "trn_labels = onehot(trn_labels)\n",
    "\n",
    "input_dim = trn_input.shape[1]\n",
    "nb_classes = trn_labels.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n"
     ]
    }
   ],
   "source": [
    "# To show how we'd know what the input dimensions should be without researching MNIST:\n",
    "print(trn_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That above is 42,0000 images by 784 pixels. So, the usual 28x28 pixel images. We can do the same to take a look at the output, which, not surprisingly, is the 10 possible digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(trn_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I/O Dimensions: determined by data/categories/network\n",
    "# Output_Cols = 10\n",
    "# input_dim  = 784 # for 1st layer only: rest do auto-shape-inference\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 0.1\n",
    "optz = SGD(lr=LR)\n",
    "# optz = RMSprop(lr=LR)\n",
    "lossFn = 'mse'\n",
    "# lossFn = 'categorical_cross_entropy'\n",
    "metric=['accuracy']\n",
    "# metrics=None\n",
    "\n",
    "LM = Sequential( [Dense(nb_classes, input_shape=(input_dim,))] )\n",
    "# LM.compile(optmizer = optz, loss = lossFn, metrics = metric)\n",
    "LM.compile(optimizer=SGD(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# lm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 4s - loss: 11.4692 - acc: 0.1761      ETA: 3s - los - ETA - ETA: 1s - loss: - ETA: 0s - loss: 11.4513 - acc:  - ETA: 0s - los\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 4s - loss: 11.4692 - acc: 0.1761      ETA: 1s - loss: 1 - ETA: 0s - l\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 4s - loss: 11.4692 - acc: 0.1761     \n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 4s - loss: 11.4692 - acc: 0.1761      ETA - ETA: 1s - loss: 11.4770 - acc: \n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 4s - loss: 11.4692 - acc: 0.1761      ETA: 0s - loss: 11.4725 - acc: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10c410390>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the data\n",
    "LM.fit(trn_input, trn_labels, nb_epoch=5, batch_size = 4, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I'd expect, a single perceptron layer doing a linear mapping between input to output performs.. poorly. But this is one of the first times I'm hand coding this from scratch, and it's good to be in a place where I can start experimenting without spending the bulk of mental effort over getting the machine to work.\n",
    "\n",
    "By accidentally running more epochs without re-initializing the model, it seems it plateaus at ```0.1761``` accuracy.\n",
    "\n",
    "It'll be interesting to see how RMSprop compares to SGD, and mean-squared error vs categorical cross-entropy. More interesting is adding more layers and tweaking their activations and looking at different learning rates: constant or graduated. *Even* more interesting is adding backpropagation and turning it into a neural network.\n",
    "\n",
    "Below I'm getting the data separated into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Turns out this cell was unnecessary\n",
    "# import pandas as pd\n",
    "# test_data = pd.read_csv(path + 'test.csv')\n",
    "# # wait there are no labels that's the point..\n",
    "# # test_labels = test_data.ix[:,0].values.astype('int32')\n",
    "# test_input  = (test_data.ix[:,1:].values).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 784)\n",
      "(28000,)\n"
     ]
    }
   ],
   "source": [
    "# print(test_data.shape)\n",
    "# print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately I don't yet know how to separate a .csv file into a validation set. Wait that sounds easy. Take a random permutation, or for the lazy: the first X amount of inputs and labels from the training set and call that validation. Oh. Okay. Maybe I should do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "(42000, 784)\n",
      "(42000, 10)\n"
     ]
    }
   ],
   "source": [
    "# # Wondering if a crash I had earlier was because One-Hotting did something to the labels..\n",
    "# trn_data = pd.read_csv(path + 'train.csv')\n",
    "# trn_labels = trn_data.ix[:,0].values.astype('int32')\n",
    "# trn_input = (trn_data.ix[:,1:].values).astype('float32')\n",
    "\n",
    "\n",
    "# test_data has 42,000 elements. I'll take 2,000 for validation.\n",
    "# val_data = trn_data[:2000]\n",
    "# trn_2_data = trn_data[2000:]\n",
    "\n",
    "# val_input  = val_data.ix[:,0].values.astype('int32')\n",
    "# val_labels = (val_data.ix[:,1:].values).astype('float32')\n",
    "\n",
    "# trn_2_input = trn_2_data.ix[:,0].values.astype('int32')\n",
    "# trn_2_labels = (trn_2_data.ix[:,1:].values).astype('float32')\n",
    "\n",
    "\n",
    "# trying to do this in a way that doesn't kill the kernel\n",
    "print(trn_data.shape)\n",
    "print(trn_input.shape)\n",
    "print(trn_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so.. training data is the number of images X (number of pixels + label). The label just adds 1 to the vector's length because it's just a decimal {0..9}, giving 42k X 785..\n",
    "\n",
    "The training input vector has the label removed so its 42k images X 784 pixels..\n",
    "\n",
    "The trianing labels vector is one-hot encoded to a ten-bit, & of course the 42k images..\n",
    "\n",
    "So... nothing is saying I can't just cut input & labels and separate those into new training and validation sets. Not sure why I was getting crashes the other way, but we'll see if this works (it should). Ooo, maybe I.. okay maybe I made a mistake with leaving the labels on or something, before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2000, 785), (2000,), (2000, 784))\n",
      "((40000,), (40000,), (40000, 784))\n"
     ]
    }
   ],
   "source": [
    "# Old\n",
    "# print(val_data.shape, val_input.shape, val_labels.shape)\n",
    "# print(trn_2_input.shape, trn_2_input.shape, trn_2_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2000, 784), (2000, 10))\n",
      "((40000, 784), (40000, 10))\n"
     ]
    }
   ],
   "source": [
    "val_input  = trn_input[:2000]\n",
    "val_labels = trn_labels[:2000]\n",
    "\n",
    "newtrn_input = trn_input[2000:]\n",
    "newtrn_labels = trn_labels[2000:]\n",
    "\n",
    "print(val_input.shape, val_labels.shape)\n",
    "print(newtrn_input.shape, newtrn_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we got those separated.. let's do the same thing as before: single Linear Model Perceptron Layer, but with a validation set to check against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The stuff above's One-Hotted.. so don't do it again..\n",
    "\n",
    "# # I forgot the onehot encode the labels after loading from disk\n",
    "# val_labels = onehot(val_labels)\n",
    "# trn_2_labels = onehot(trn_2_labels)\n",
    "# print(val_labels.shape)\n",
    "# print(trn_2_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "40000/40000 [==============================] - 4s - loss: nan - acc: 0.0983 - val_loss: nan - val_acc: 0.0980\n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 4s - loss: nan - acc: 0.0984 - val_loss: nan - val_acc: 0.0980\n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 4s - loss: nan - acc: 0.0984 - val_loss: nan - val_acc: 0.0980\n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 4s - loss: nan - acc: 0.0984 - val_loss: nan - val_acc: 0.0980\n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 5s - loss: nan - acc: 0.0984 - val_loss: nan - val_acc: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b16a390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM = Sequential([Dense(nb_classes, input_dim=input_dim)])\n",
    "LM.compile(optimizer='sgd', loss='mse', metrics=['accuracy'])\n",
    "# LM = Sequential([Dense(nb_classes, activation='sigmoid', input_dim=input_dim)])\n",
    "\n",
    "LM.fit(newtrn_input, newtrn_labels, nb_epoch=5, batch_size=4,\n",
    "       validation_data=(val_input, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 4s - loss: 0.0983 - acc: 0.1568     - ETA: 4s - loss: 0.1200 - acc: 0.099 - ETA: 4s - loss: 0.1189 - acc - ETA: 3s - ETA: 0s - loss: 0.0985 - acc: \n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 5s - loss: 0.0933 - acc: 0.1843     - ETA: 4s - loss: 0.0932 - - ETA: 3s - loss:  - ETA: 0s - loss: 0.0934 - acc: 0.183 - ETA: 0s - loss: 0.0934 - acc: 0 - ETA: 0s - loss: 0.0934 - \n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 4s - loss: 0.0930 - acc: 0.1920     - ETA: 3s - loss: 0.0927 - acc: 0 - ETA: 3s - loss: 0.0927 - - E - ETA: 0s - loss: 0.0\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 4s - loss: 0.0928 - acc: 0.1877     - ET - ETA: 0s - loss: 0.0928 \n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 5s - loss: 0.0925 - acc: 0.1981     - ETA: 3s - loss: 0.0929 - - ETA:  - ETA: 1s - loss: 0.0926 - a - ETA: 0s - loss: 0.0926 - \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x118ad3410>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM2 = Sequential([Dense(nb_classes, activation='sigmoid', input_dim=input_dim)])\n",
    "LM2.compile(optimizer='sgd', loss='mse', metrics=['accuracy'])\n",
    "LM2.fit(trn_input, trn_labels, nb_epoch=5, batch_size=4, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, I'm not seeing a real difference in not/using an activation function for just a single linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Multilayer Perceptron (MLP) can be built by simple adding on layers. The big difference between an MLP and a NN is no backpropagation is going on. A single forward pass is done through the network each epoch, and there isn't any adjustment of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this notebook will be a bit of a mess; the machine isn't the only one learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "40000/40000 [==============================] - 14s - loss: 0.0947 - acc: 0.1021 - val_loss: 0.0900 - val_acc: 0.1085\n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 14s - loss: 0.0933 - acc: 0.1021 - val_loss: 0.0900 - val_acc: 0.1100\n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 13s - loss: 0.0928 - acc: 0.1005 - val_loss: 0.0900 - val_acc: 0.1155\n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 14s - loss: 0.0924 - acc: 0.1002 - val_loss: 0.0900 - val_acc: 0.1655\n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 14s - loss: 0.0919 - acc: 0.1040 - val_loss: 0.0899 - val_acc: 0.1085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11be078d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just here so I have them infront of me\n",
    "input_dim = input_dim # 784\n",
    "nb_classes = nb_classes # 10\n",
    "\n",
    "MLP = Sequential()\n",
    "MLP.add(Dense(112, input_dim=input_dim)) # I'll just set internal output to 4x28=112\n",
    "MLP.add(Activation('sigmoid'))\n",
    "# I dont know what to set the dropout layer too, but I see it in keras.io as 0.5\n",
    "MLP.add(Dropout(0.5))\n",
    "\n",
    "# and now to add 3 more layers set to sigmoid\n",
    "for layer in xrange(3):\n",
    "    MLP.add(Dense(112))\n",
    "    MLP.add(Activation('sigmoid'))\n",
    "    MLP.add(Dropout(0.5))\n",
    "\n",
    "# and our final layers\n",
    "MLP.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# this will be a beautiful disaster\n",
    "MLP.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "MLP.fit(newtrn_input, newtrn_labels, nb_epoch=5, batch_size=4,\n",
    "        validation_data = (val_input, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
