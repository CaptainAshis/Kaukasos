{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New 'clean-pass' of L3HW-SF ~ usin' lessons learned\n",
    "\n",
    "Wayne Nixalo - 2017-May-23 02:37\n",
    "\n",
    "useful links:\n",
    "DataAugmentation:\n",
    "https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson3.ipynb\n",
    "\n",
    "I forgot but reference anyway:\n",
    "https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson2.ipynb\n",
    "\n",
    "Good followthru of lecture & how to save to submission w/ Pandas:\n",
    "https://github.com/philippbayer/cats_dogs_redux/blob/master/Statefarm.ipynb\n",
    "\n",
    "Me:\n",
    "https://github.com/WNoxchi/Kaukasos/blob/master/FAI/lesson3/L3HW_SF.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 870M (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import bcolz\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "sys.path.insert(1, os.path.join(os.getcwd(), '../utils'))\n",
    "import utils\n",
    "from vgg16bn import Vgg16BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR  = os.getcwd()\n",
    "DATA_DIR  = HOME_DIR + '/data'\n",
    "TEST_DIR  = DATA_DIR + '/test'\n",
    "TRAIN_DIR = DATA_DIR + '/train'\n",
    "VALID_DIR = DATA_DIR + '/valid'\n",
    "\n",
    "data_path    = DATA_DIR  + '/'\n",
    "test_path    = TEST_DIR  + '/'\n",
    "train_path   = TRAIN_DIR + '/'\n",
    "valid_path   = VALID_DIR + '/'\n",
    "results_path = DATA_DIR  + '/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]\n",
    "\n",
    "def reset_valid(verbose=1):\n",
    "    \"\"\"Moves all images in validation set back to \n",
    "    their respective classes in the training set.\"\"\"\n",
    "    counter = 0\n",
    "    %cd $valid_path\n",
    "    for i in xrange(10):\n",
    "        %cd c\"$i\"\n",
    "        g = glob('*.jpg')\n",
    "        for n in xrange(len(g)):\n",
    "            os.rename(g[n], TRAIN_DIR + '/c' + str(i) + '/' + g[n])\n",
    "            counter += 1\n",
    "        %cd ..\n",
    "    if verbose: print(\"Moved {} files\".format(counter))\n",
    "\n",
    "# modified from: http://forums.fast.ai/t/statefarm-kaggle-comp/183/20\n",
    "def set_valid(number=1, verbose=1):\n",
    "    \"\"\"Moves <number> subjects from training to validation \n",
    "    directories. Verbosity 0: Silent; 1: print no. files moved;\n",
    "    2: print each move operation. Default=1\"\"\"\n",
    "    if number < 0: number = 0\n",
    "    # repeat for <number> subjects\n",
    "    for n in xrange(number):\n",
    "        # read CSV file into Pandas DataFrame\n",
    "        dil = pd.read_csv(data_path + 'driver_imgs_list.csv')\n",
    "        # grouped frame by subject in image\n",
    "        grouped_subjects = dil.groupby('subject')\n",
    "        # pick subject at random\n",
    "        subject = grouped_subjects.groups.keys()[np.random.randint(0, \\\n",
    "                                         high=len(grouped_subjects.groups)-1)]\n",
    "        # get group assoc w/ subject\n",
    "        group = grouped_subjects.get_group(subject)\n",
    "        # loop over group & move imgs to validation dir\n",
    "        counter = 0\n",
    "        for (subject, clssnm, img) in group.values:\n",
    "            source = '{}train/{}/{}'.format(data_path, clssnm, img)\n",
    "            target = source.replace('train', 'valid')\n",
    "            if verbose > 1: print('mv {} {}'.format(source, target))\n",
    "            os.rename(source, target)\n",
    "            counter += 1\n",
    "        if verbose: print(\"Files moved: {}\".format(counter))\n",
    "# function to build FCNet w/ BatchNormalization & Dropout\n",
    "def create_FCbn_layers(p=0):\n",
    "    return [\n",
    "            MaxPooling2D(input_shape=Conv_model[-1].output_shape[1:]),\n",
    "            Flatten(),\n",
    "            BatchNormalization(),\n",
    "            Dense(4096, activation='relu'),\n",
    "            Dropout(p),\n",
    "            Dense(10, activation='softmax')\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating validation directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.mkdir(VAL_DIR)\n",
    "# for i in xrange(10):\n",
    "#     os.mkdir(VAL_DIR + '/c' + str(i))\n",
    "\n",
    "# # # another way to do this:\n",
    "# # %mkdir $VAL_DIR\n",
    "# # for i in xrange(10):\n",
    "# #     %mkdir $VAL_DIR/c\"$i\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting/resetting validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c0\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c1\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c2\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c3\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c4\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c5\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c6\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c7\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c8\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c9\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "Moved 3504 files\n",
      "Files moved: 1226\n",
      "Files moved: 876\n",
      "Files moved: 848\n"
     ]
    }
   ],
   "source": [
    "reset_valid()\n",
    "set_valid(number=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/valid batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19474 images belonging to 10 classes.\n",
      "Found 2950 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, \n",
    "                                height_shift_range=0.05, zoom_range=0.1, \n",
    "                                   shear_range=0.1, channel_shift_range=10)\n",
    "# does it matter that I don't set dim_ordering='tf'?\n",
    "trn_batches = gen.flow_from_directory(train_path, target_size=target_size, \n",
    "                batch_size=batch_size, shuffle=True, class_mode='categorical')\n",
    "val_batches = gen.flow_from_directory(valid_path, target_size=target_size, \n",
    "                batch_size=batch_size, shuffle=False, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VGG16BN model & its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VGGbn = Vgg16BN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just learned that the ```utils.vgg_ft_bn(num)``` function called in the JNB I was referencing is just initializes a model, and finetunes it in the standard way: pop off last layer, set all remaining layers to un-trainable, add a single softmax output FC layer, and compile. So as an experiment... how *bad* of an idea would, say, doing that & training the output layer *then* training all layers including convolutionals, be?\n",
    "\n",
    "That kind of messes with the cleaned-up flow of this notebook, but that's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finetune the model - same as utils.vgg_ft_bn(num)\n",
    "VGGbn.model.pop()\n",
    "for layer in VGGbn.model.layers: layer.trainable=False\n",
    "VGGbn.model.add(Dense(10, activation='softmax'))\n",
    "VGGbn.model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# I should be able to get around the multiple-model memory issue by saving a set \n",
    "# of 'stock' weights & resetting the VGG16bn model on ea. iteration of the ensemble.\n",
    "# VGGbn.model.save_weights(data_path + 'vgg16bn_stock.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model at low η to adjust Conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many epochs at what η to do this until optimal\n",
    "VGGbn.model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, \n",
    "                    validation_data=val_batches, nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGbn.model.optimizer.lr=1e-3\n",
    "VGGbn.model.fit_generator(trn_batches, trn_batches.n, nb_epoch=2,\n",
    "                          validation_data=val_batches, nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGbn.model.optimizer.lr=1e-6\n",
    "VGGbn.model.fit_generator(trn_batches, trn_batches.n, nb_epoch=3,\n",
    "                          validation_data=val_batches, nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in VGGbn.model.layers: layer.trainable=True\n",
    "VGGbn.model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1,\n",
    "                          validation_data=val_batches, nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "    1/19474 [..............................] - ETA: 11968s - loss: 2.5765 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "GpuArrayException",
     "evalue": "out of memory\nApply node that caused the error: GpuDot22(InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{((i0 * Composite{(i0 + (i1 * i2))}(i1, i2, i3)) + (i4 * Composite{(i0 + (i1 * i2))}(i1, i2, i3) * sgn(i5)))}}[(0, 1)]<gpuarray>.0)\nToposort index: 829\nInputs types: [GpuArrayType<None>(float32, (False, False)), GpuArrayType<None>(float32, (False, False))]\nInputs shapes: [(25088, 1), (1, 4096)]\nInputs strides: [(4, 100352), (16384, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Composite{((i0 * i1) + (i2 * sqr(i3)))}}[(0, 1)]<gpuarray>(InplaceGpuDimShuffle{x,x}.0, <GpuArrayType<None>(float32, (False, False))>, InplaceGpuDimShuffle{x,x}.0, GpuDot22.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGpuArrayException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e2a4203f489b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mVGGbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m VGGbn.model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1,\n\u001b[0;32m----> 5\u001b[0;31m                           validation_data=val_batches, nb_val_samples=val_batches.n)\n\u001b[0m",
      "\u001b[0;32m/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpygpu/gpuarray.pyx\u001b[0m in \u001b[0;36mpygpu.gpuarray.pygpu_empty (pygpu/gpuarray.c:9764)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpygpu/gpuarray.pyx\u001b[0m in \u001b[0;36mpygpu.gpuarray.array_empty (pygpu/gpuarray.c:5616)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mGpuArrayException\u001b[0m: out of memory\nApply node that caused the error: GpuDot22(InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{((i0 * Composite{(i0 + (i1 * i2))}(i1, i2, i3)) + (i4 * Composite{(i0 + (i1 * i2))}(i1, i2, i3) * sgn(i5)))}}[(0, 1)]<gpuarray>.0)\nToposort index: 829\nInputs types: [GpuArrayType<None>(float32, (False, False)), GpuArrayType<None>(float32, (False, False))]\nInputs shapes: [(25088, 1), (1, 4096)]\nInputs strides: [(4, 100352), (16384, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Composite{((i0 * i1) + (i2 * sqr(i3)))}}[(0, 1)]<gpuarray>(InplaceGpuDimShuffle{x,x}.0, <GpuArrayType<None>(float32, (False, False))>, InplaceGpuDimShuffle{x,x}.0, GpuDot22.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# that didn't take any longer. what if I compile then train?\n",
    "for layer in VGGbn.model.layers: layer.trainable=True\n",
    "VGGbn.model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "VGGbn.model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1,\n",
    "                          validation_data=val_batches, nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Conv layers & create new ConvNet (w/ vgg weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_conv_idx = [index, for index, layer in enumerate(VGGbn.model.layers) \\\n",
    "                                            if type(layer) is Convolution2D][-1]\n",
    "Conv_layers = VGGbn.model.layers[:last_conv_idx + 1]\n",
    "Conv_model = Sequential(Conv_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training batches have to be set to not be shuffled. Since the full-model is in two stages, classes & labels will be supplied to the FCNet via the batches-generator; if these are shuffled they won't match up with the output features from the ConvNet. I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_batches = gen.flow_from_directory(train_path, target_size=target_size, \n",
    "                batch_size=batch_size, shuffle=False, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Conv Model on trn/val batches to create features as inputs to FCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_features = Conv_model.predict_generator(trn_batches, trn_batches.nb_sample)\n",
    "conv_val_feat = Conv_model.predict_generator(val_batches, val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optional: save the convolutional model's output features\n",
    "# save_array(results_path + 'conv_features.dat', conv_features)\n",
    "# save_array(results_path + 'conv_val_feat.dat', conv_val_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Fully-Connected Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FC_model = Sequential(create_FCbn_layers(p=0.3))\n",
    "FC_model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train FCNet on ConvNet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FC_model.fit(conv_features, trn_batches.labels, batch_size=batch_size, \n",
    "             nb_epoch=1, validation_data=(conv_val_feat, val_batches.labels))\n",
    "# is there a way to do this as a generator -- or does it not matter?\n",
    "# maybe save the features, then pull them from disk in batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Augmented batch generator for test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGeneratro()\n",
    "tst_batches = gen.flow_from_directory(test_path, batch_size=batch_size,\n",
    "                                      shuffle=False, class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run test batches through ConvNet, run ConvNet test features through FCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_tst_feat = Conv_model.predict_generator(tst_batches, tst_batches.nb_sample)\n",
    "preds = FC_model.predict(conv_tst_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = tst_batches.filenames\n",
    "classes = sorted(trn_batches.class_indices, key=trn_batches.class_indices.get)\n",
    "submission = pd.DataFrame(preds, columns=classes)\n",
    "submission.insert(0, 'img', [f[8:] for f in filenames])\n",
    "submission.head()\n",
    "submission.to_csv(results_path + 'submission.csv', index=False, compression=None)\n",
    "\n",
    "from IPython.display import FileLink\n",
    "FileLink(results_path + 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above is working:\n",
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
