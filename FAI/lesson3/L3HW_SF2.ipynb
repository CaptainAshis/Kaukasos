{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New 'clean-pass' of L3HW-SF ~ usin' lessons learned\n",
    "\n",
    "Wayne Nixalo - 2017-May-23 02:37\n",
    "\n",
    "useful links:\n",
    "DataAugmentation:\n",
    "https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson3.ipynb\n",
    "\n",
    "I forgot but reference anyway:\n",
    "https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson2.ipynb\n",
    "\n",
    "Good followthru of lecture & how to save to submission w/ Pandas:\n",
    "https://github.com/philippbayer/cats_dogs_redux/blob/master/Statefarm.ipynb\n",
    "\n",
    "Me:\n",
    "https://github.com/WNoxchi/Kaukasos/blob/master/FAI/lesson3/L3HW_SF.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 870M (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import bcolz\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "sys.path.insert(1, os.path.join(os.getcwd(), '../utils'))\n",
    "import utils\n",
    "from vgg16bn import Vgg16BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR  = os.getcwd()\n",
    "DATA_DIR  = HOME_DIR + '/data'\n",
    "TEST_DIR  = DATA_DIR + '/test'\n",
    "TRAIN_DIR = DATA_DIR + '/train'\n",
    "VALID_DIR = DATA_DIR + '/valid'\n",
    "\n",
    "data_path    = DATA_DIR  + '/'\n",
    "test_path    = TEST_DIR  + '/'\n",
    "train_path   = TRAIN_DIR + '/'\n",
    "valid_path   = VALID_DIR + '/'\n",
    "results_path = DATA_DIR  + '/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]\n",
    "\n",
    "def reset_valid(verbose=1):\n",
    "    \"\"\"Moves all images in validation set back to \n",
    "    their respective classes in the training set.\"\"\"\n",
    "    counter = 0\n",
    "    %cd $valid_path\n",
    "    for i in xrange(10):\n",
    "        %cd c\"$i\"\n",
    "        g = glob('*.jpg')\n",
    "        for n in xrange(len(g)):\n",
    "            os.rename(g[n], TRAIN_DIR + '/c' + str(i) + '/' + g[n])\n",
    "            counter += 1\n",
    "        %cd ..\n",
    "    if verbose: print(\"Moved {} files\".format(counter))\n",
    "\n",
    "# modified from: http://forums.fast.ai/t/statefarm-kaggle-comp/183/20\n",
    "def set_valid(number=1, verbose=1):\n",
    "    \"\"\"Moves <number> subjects from training to validation \n",
    "    directories. Verbosity 0: Silent; 1: print no. files moved;\n",
    "    2: print each move operation. Default=1\"\"\"\n",
    "    if number < 0: number = 0\n",
    "    # repeat for <number> subjects\n",
    "    for n in xrange(number):\n",
    "        # read CSV file into Pandas DataFrame\n",
    "        dil = pd.read_csv(data_path + 'driver_imgs_list.csv')\n",
    "        # grouped frame by subject in image\n",
    "        grouped_subjects = dil.groupby('subject')\n",
    "        # pick subject at random\n",
    "        subject = grouped_subjects.groups.keys()[np.random.randint(0, \\\n",
    "                                         high=len(grouped_subjects.groups)-1)]\n",
    "        # get group assoc w/ subject\n",
    "        group = grouped_subjects.get_group(subject)\n",
    "        # loop over group & move imgs to validation dir\n",
    "        counter = 0\n",
    "        for (subject, clssnm, img) in group.values:\n",
    "            source = '{}train/{}/{}'.format(data_path, clssnm, img)\n",
    "            target = source.replace('train', 'valid')\n",
    "            if verbose > 1: print('mv {} {}'.format(source, target))\n",
    "            os.rename(source, target)\n",
    "            counter += 1\n",
    "        if verbose: print(\"Files moved: {}\".format(counter))\n",
    "# function to build FCNet w/ BatchNormalization & Dropout\n",
    "def create_FCbn_layers(p=0):\n",
    "    return [\n",
    "            MaxPooling2D(input_shape=Conv_model.layers[-1].output_shape[1:]),\n",
    "#             MaxPooling2D(),\n",
    "            Flatten(),\n",
    "            BatchNormalization(),\n",
    "            Dense(4096, activation='relu'),\n",
    "            Dropout(p),\n",
    "            Dense(10, activation='softmax')\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating validation directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.mkdir(VAL_DIR)\n",
    "# for i in xrange(10):\n",
    "#     os.mkdir(VAL_DIR + '/c' + str(i))\n",
    "\n",
    "# # # another way to do this:\n",
    "# # %mkdir $VAL_DIR\n",
    "# # for i in xrange(10):\n",
    "# #     %mkdir $VAL_DIR/c\"$i\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting/resetting validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c0\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c1\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c2\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c3\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c4\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c5\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c6\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c7\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c8\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c9\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "Moved 1869 files\n",
      "Files moved: 790\n",
      "Files moved: 591\n",
      "Files moved: 1196\n"
     ]
    }
   ],
   "source": [
    "reset_valid()\n",
    "set_valid(number=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/valid batch generators\n",
    "\n",
    "Training batches have to be set to not be shuffled. Since the full-model is in two stages, classes & labels will be supplied to the FCNet via the batches-generator; if these are shuffled they won't match up with the output features from the ConvNet. I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19847 images belonging to 10 classes.\n",
      "Found 2577 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, \n",
    "                                height_shift_range=0.05, zoom_range=0.1, \n",
    "                                   shear_range=0.1, channel_shift_range=10)\n",
    "# does it matter that I don't set dim_ordering='tf'?\n",
    "trn_batches = gen.flow_from_directory(train_path, target_size=target_size, \n",
    "                batch_size=batch_size, shuffle=False, class_mode='categorical')\n",
    "val_batches = gen.flow_from_directory(valid_path, target_size=target_size, \n",
    "                batch_size=batch_size, shuffle=False, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??utils.get_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VGG16BN model & its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VGGbn = Vgg16BN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGbn.model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just learned that the ```utils.vgg_ft_bn(num)``` function called in the JNB I was referencing is just initializes a model, and finetunes it in the standard way: pop off last layer, set all remaining layers to un-trainable, add a single softmax output FC layer, and compile. So as an experiment... how *bad* of an idea would, say, doing that & training the output layer *then* training all layers including convolutionals, be?\n",
    "\n",
    "That kind of messes with the cleaned-up flow of this notebook, but that's fine.\n",
    "\n",
    "Answer: *it's pretty bad*. Even with a batch size of ```1``` the GPU still runs out of memory, if barely. Perhaps using non-augmented data would spare a bit of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Conv layers & create new ConvNet (w/ vgg weights)\n",
    "\n",
    "Since I'm generating randomly-augmented data each batch, I can't precompute the Conv features. I'd have to experiment and see the speed vs. accuracy tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_idx = [index for index, layer in enumerate(VGGbn.model.layers) \\\n",
    "                                            if type(layer) is Convolution2D][-1]\n",
    "Conv_layers = VGGbn.model.layers[:last_conv_idx + 1]\n",
    "Conv_model = Sequential(Conv_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Conv Model on trn/val batches to create features as inputs to FCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_features = Conv_model.predict_generator(trn_batches, trn_batches.nb_sample)\n",
    "conv_val_feat = Conv_model.predict_generator(val_batches, val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_features = load_array(results_path + 'conv_features.bc')\n",
    "conv_val_feat = load_array(results_path + 'conv_val_feat.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can save the features at this point. For use later, and also to pull them in batches (use ```image.ImageDataGenerator()``` and ```gen.flow_from_directory(shuffle=False)```. But I'll already have them all in memory when the convolutional model is done, and I'm going to run a differently-randomly-augmented batch of (unshuffled) data through each time, so there isn't a reason to precompute & save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # optional: save the convolutional model's output features\n",
    "# save_array(results_path + 'conv_features.dat', conv_features)\n",
    "# save_array(results_path + 'conv_val_feat.dat', conv_val_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Fully-Connected Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_model = Sequential(create_FCbn_layers(p=0.3))\n",
    "FC_model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19847 images belonging to 10 classes.\n",
      "Found 2577 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    validation_filenames, training_filenames, testing_filenames) = utils.get_classes(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(results_path + 'conv_features.bc', conv_features)\n",
    "save_array(results_path + 'conv_val_feat.bc', conv_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??utils.get_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train FCNet on ConvNet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19847 samples, validate on 2577 samples\n",
      "Epoch 1/1\n",
      "19847/19847 [==============================] - 81s - loss: 7.5692 - acc: 0.5260 - val_loss: 8.2475 - val_acc: 0.4835\n",
      "Train on 19847 samples, validate on 2577 samples\n",
      "Epoch 1/1\n",
      "19847/19847 [==============================] - 81s - loss: 7.1666 - acc: 0.5529 - val_loss: 8.7738 - val_acc: 0.4497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f13a8759e10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each epoch on GTX870M ~18 minutes\n",
    "# wow, no, ~18 min for CNN. FCNet only is ~ 1 minute!\n",
    "# FC_model.fit(conv_features, trn_labels, batch_size=batch_size, \n",
    "#              nb_epoch=1, validation_data=(conv_val_feat, val_labels))\n",
    "FC_model.optimizer.lr=1e-1\n",
    "FC_model.fit(conv_features, trn_labels, batch_size=batch_size, \n",
    "             nb_epoch=1, validation_data=(conv_val_feat, val_labels))\n",
    "FC_model.optimizer.lr=1e-2\n",
    "FC_model.fit(conv_features, trn_labels, batch_size=batch_size, \n",
    "             nb_epoch=1, validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??utils.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19847 samples, validate on 2577 samples\n",
      "Epoch 1/4\n",
      "19847/19847 [==============================] - 81s - loss: 6.6582 - acc: 0.5843 - val_loss: 7.9138 - val_acc: 0.5072\n",
      "Epoch 2/4\n",
      "19847/19847 [==============================] - 81s - loss: 6.6488 - acc: 0.5862 - val_loss: 7.4051 - val_acc: 0.5382\n",
      "Epoch 3/4\n",
      "19847/19847 [==============================] - 81s - loss: 6.9273 - acc: 0.5688 - val_loss: 7.5799 - val_acc: 0.5289\n",
      "Epoch 4/4\n",
      "19847/19847 [==============================] - 81s - loss: 6.3319 - acc: 0.6065 - val_loss: 7.5594 - val_acc: 0.5301\n",
      "Train on 19847 samples, validate on 2577 samples\n",
      "Epoch 1/8\n",
      "19847/19847 [==============================] - 81s - loss: 6.3226 - acc: 0.6069 - val_loss: 8.6045 - val_acc: 0.4649\n",
      "Epoch 2/8\n",
      "19847/19847 [==============================] - 81s - loss: 6.0437 - acc: 0.6240 - val_loss: 7.9359 - val_acc: 0.5072\n",
      "Epoch 3/8\n",
      "19847/19847 [==============================] - 80s - loss: 6.2177 - acc: 0.6134 - val_loss: 7.5561 - val_acc: 0.5305\n",
      "Epoch 4/8\n",
      "19847/19847 [==============================] - 80s - loss: 6.2718 - acc: 0.6100 - val_loss: 8.3782 - val_acc: 0.4796\n",
      "Epoch 5/8\n",
      "19847/19847 [==============================] - 80s - loss: 6.3415 - acc: 0.6058 - val_loss: 7.9296 - val_acc: 0.5068\n",
      "Epoch 6/8\n",
      "19847/19847 [==============================] - 80s - loss: 6.0521 - acc: 0.6238 - val_loss: 7.7263 - val_acc: 0.5204\n",
      "Epoch 7/8\n",
      "19847/19847 [==============================] - 80s - loss: 5.8950 - acc: 0.6339 - val_loss: 8.1752 - val_acc: 0.4924\n",
      "Epoch 8/8\n",
      "19847/19847 [==============================] - 80s - loss: 5.9502 - acc: 0.6304 - val_loss: 7.6494 - val_acc: 0.5250\n",
      "Train on 19847 samples, validate on 2577 samples\n",
      "Epoch 1/12\n",
      "19847/19847 [==============================] - 80s - loss: 6.1565 - acc: 0.6176 - val_loss: 7.6712 - val_acc: 0.5231\n",
      "Epoch 2/12\n",
      "19847/19847 [==============================] - 81s - loss: 6.3666 - acc: 0.6046 - val_loss: 7.6187 - val_acc: 0.5270\n",
      "Epoch 3/12\n",
      "19847/19847 [==============================] - 80s - loss: 6.1146 - acc: 0.6200 - val_loss: 7.3156 - val_acc: 0.5460\n",
      "Epoch 4/12\n",
      "19847/19847 [==============================] - 81s - loss: 6.1163 - acc: 0.6200 - val_loss: 7.1232 - val_acc: 0.5576\n",
      "Epoch 5/12\n",
      "19847/19847 [==============================] - 80s - loss: 5.8694 - acc: 0.6355 - val_loss: 6.8897 - val_acc: 0.5720\n",
      "Epoch 6/12\n",
      "19847/19847 [==============================] - 80s - loss: 6.0104 - acc: 0.6267 - val_loss: 7.5773 - val_acc: 0.5297\n",
      "Epoch 7/12\n",
      "19847/19847 [==============================] - 80s - loss: 6.0409 - acc: 0.6247 - val_loss: 7.8233 - val_acc: 0.5142\n",
      "Epoch 8/12\n",
      "19847/19847 [==============================] - 80s - loss: 5.7253 - acc: 0.6445 - val_loss: 6.7477 - val_acc: 0.5813\n",
      "Epoch 9/12\n",
      "19847/19847 [==============================] - 80s - loss: 5.7190 - acc: 0.6449 - val_loss: 8.2757 - val_acc: 0.4858\n",
      "Epoch 10/12\n",
      "19847/19847 [==============================] - 80s - loss: 5.8177 - acc: 0.6389 - val_loss: 8.5814 - val_acc: 0.4676\n",
      "Epoch 11/12\n",
      "19847/19847 [==============================] - 81s - loss: 5.7796 - acc: 0.6412 - val_loss: 7.8501 - val_acc: 0.5126\n",
      "Epoch 12/12\n",
      "19847/19847 [==============================] - 81s - loss: 6.0719 - acc: 0.6231 - val_loss: 8.4187 - val_acc: 0.4777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f13a8704bd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import to_categorical as onehot\n",
    "FC_model.optimizer.lr=1e-4\n",
    "FC_model.fit(conv_features, onehot(trn_batches.classes), batch_size=batch_size, \n",
    "             nb_epoch=4, validation_data=(conv_val_feat, onehot(val_batches.classes)))\n",
    "FC_model.optimizer.lr=1e-4\n",
    "FC_model.fit(conv_features, onehot(trn_batches.classes), batch_size=batch_size, \n",
    "             nb_epoch=8, validation_data=(conv_val_feat, onehot(val_batches.classes)))\n",
    "FC_model.optimizer.lr=1e-6\n",
    "FC_model.fit(conv_features, onehot(trn_batches.classes), batch_size=batch_size, \n",
    "             nb_epoch=12, validation_data=(conv_val_feat, onehot(val_batches.classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Augmented batch generator for test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGeneratro()\n",
    "tst_batches = gen.flow_from_directory(test_path, batch_size=batch_size,\n",
    "                                      shuffle=False, class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run test batches through ConvNet, run ConvNet test features through FCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_tst_feat = Conv_model.predict_generator(tst_batches, tst_batches.nb_sample)\n",
    "preds = FC_model.predict(conv_tst_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = tst_batches.filenames\n",
    "classes = sorted(trn_batches.class_indices, key=trn_batches.class_indices.get)\n",
    "submission = pd.DataFrame(preds, columns=classes)\n",
    "submission.insert(0, 'img', [f[8:] for f in filenames])\n",
    "submission.head()\n",
    "submission.to_csv(results_path + 'submission.csv', index=False, compression=None)\n",
    "\n",
    "from IPython.display import FileLink\n",
    "FileLink(results_path + 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above is working:\n",
    "### Ensemble\n",
    "\n",
    "Work-flow:\n",
    "\n",
    "To save GPU memory (no easy way to free it yet in JNB) init a *single* Convolutional and Fully-Connected Net model. The convolutional layer weights will not be trained, only used to produce features, so only the FC-model will have it's weights saved/reinitialized (if I can re-init without increasing memory load, I'll do that).\n",
    "\n",
    "For each iteration of the ensemble, a randomly-augmented set of data will be passed through the ConvNet, and those features will be used to train a fresh FC Net. After the training phase, The Conv & FC nets will be used to create a set of predictions which will be stored in an array and returned. That array is then averaged into a single list of predictions, clipped & renormalized, then saved to be uploaded as the final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_FCNet():\n",
    "    FC_model.fit(conv_features, trn_batches.labels, batch_size=batch_size, \n",
    "                 nb_epoch=1, validation_data=(conv_val_feat, val_batches.labels))\n",
    "    FC_model.optimizer.lr=1e-1\n",
    "    FC_model.fit(conv_features, trn_batches.labels, batch_size=batch_size, \n",
    "                 nb_epoch=1, validation_data=(conv_val_feat, val_batches.labels))\n",
    "    FC_model.optimizer.lr=1e-2\n",
    "    FC_model.fit(conv_features, trn_batches.labels, batch_size=batch_size, \n",
    "                 nb_epoch=2, validation_data=(conv_val_feat, val_batches.labels))\n",
    "    FC_model.optimizer.lr=1e-4\n",
    "    FC_model.fit(conv_features, trn_batches.labels, batch_size=batch_size, \n",
    "                 nb_epoch=4, validation_data=(conv_val_feat, val_batches.labels))\n",
    "    FC_model.optimizer.lr=1e-6\n",
    "    FC_model.fit(conv_features, trn_batches.labels, batch_size=batch_size, \n",
    "                 nb_epoch=8, validation_data=(conv_val_feat, val_batches.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Ensemble(num_models=1):\n",
    "    VGGbn = Vgg16BN()\n",
    "    last_conv_idx = [index, for index, layer in enumerate(VGGbn.model.layers) \\\n",
    "                                            if type(layer) is Convolution2D][-1]\n",
    "    Conv_layers = VGGbn.model.layers[:last_conv_idx + 1]\n",
    "    Conv_model = Sequential(Conv_layers)\n",
    "    \n",
    "    gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, \n",
    "                                       height_shift_range=0.05, zoom_range=0.1, \n",
    "                                           shear_range=0.1, channel_shift_range=10)\n",
    "    trn_batches = gen.flow_from_directory(train_path, target_size=target_size, \n",
    "                    batch_size=batch_size, shuffle=False, class_mode='categorical')\n",
    "    val_batches = gen.flow_from_directory(valid_path, target_size=target_size, \n",
    "                    batch_size=batch_size, shuffle=False, class_mode='categorical')\n",
    "    \n",
    "    gen_t = image.ImageDataGeneratro()\n",
    "    tst_batches = gen.flow_from_directory(test_path, batch_size=batch_size,\n",
    "                                          shuffle=False, class_mode=None)\n",
    "    \n",
    "    pred_array = []\n",
    "    for i in xrange(num_models):\n",
    "        # need to fix set_valid's bug\n",
    "        oserr = 1\n",
    "        while oserr:\n",
    "            oserr = 0\n",
    "            reset_valid()\n",
    "            try:\n",
    "                set_valid(number=3)\n",
    "            except OSError:\n",
    "                oserr = 1\n",
    "        \n",
    "        conv_features = Conv_model.predict_generator(trn_batches, trn_batches.nb_sample)\n",
    "        conv_val_feat = Conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "        \n",
    "        FC_model = Sequential(create_FCbn_layers(p=0.3))\n",
    "        FC_model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        train_FCNet()\n",
    "        \n",
    "        conv_tst_feat = Conv_model.predict_generator(tst_batches, tst_batches.nb_sample)\n",
    "        preds = FC_model.predict(conv_tst_feat, batch_size=batch_size*2)\n",
    "        pred_array.append(preds)\n",
    "        return pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_array = Ensemble(num_models=3)\n",
    "save_array(results_path + 'ensemble_predictions.dat', prediction_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emsemble_preds = np.stack([pred for pred in prediction_array])\n",
    "preds = ensemble_preds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = tst_batches.filenames\n",
    "classes = sorted(trn_batches.class_indices, key=trn_batches.class_indices.get)\n",
    "submission = pd.DataFrame(preds, columns=classes)\n",
    "submission.insert(0, 'img', [f[8:] for f in filenames])\n",
    "submission.head()\n",
    "submission.to_csv(results_path + 'submission.csv', index=False, compression=None)\n",
    "\n",
    "from IPython.display import FileLink\n",
    "FileLink(results_path + 'submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
