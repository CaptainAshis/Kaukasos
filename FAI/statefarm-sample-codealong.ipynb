{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter State Farm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, os.path.join(os.getcwd(), 'utils'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "# path = \"data/sample/\"\n",
    "path = \"data/statefarm/sample/\"\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample\n",
    "\n",
    "The following assumes you've already created your validation set - remember that the training and validation set should contain *different drivers*, as mentioned on the Kaggle competition page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/train\n"
     ]
    }
   ],
   "source": [
    "%cd data/statefarm\n",
    "%cd train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir ../sample\n",
    "%mkdir ../sample/train\n",
    "%mkdir ../sample/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in glob('c?'):\n",
    "    os.mkdir('../sample/train/' + d)\n",
    "    os.mkdir('../sample/valid/' + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(1500): copyfile(shuf[i], '../sample/train/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI\n"
     ]
    }
   ],
   "source": [
    "% cd ../../..\n",
    "%mkdir data/statefarm/results\n",
    "%mkdir data/statefarm/sample/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set (Sample)\n",
    "\n",
    "How I'll do it: create a full val set in the full valid folder, then copy over the same percentage as train to the sample/valid folder.\n",
    "\n",
    "Acutally: wouldn't it be better if I used the full validation set for more accurate results? Then again, for processing on my MacBook, it may be good enough to go w/ the 1st method.\n",
    "\n",
    "##### 1/3: function definitions for moving stuff & aming dirs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run once, make sure you're in datadir first\n",
    "# path = os.getcwd()\n",
    "# os.mkdir(path + '/valid')\n",
    "# for i in xrange(10): os.mkdir(path + '/valid' + '/c' + str(i))\n",
    "\n",
    "def reset_valid(verbose=1, valid_path='', TRAIN_DIR=''):\n",
    "    \"\"\"Moves all images in validation set back to \n",
    "    their respective classes in the training set.\"\"\"\n",
    "    counter = 0\n",
    "    if not valid_path: valid_path = os.getcwd() + '/valid/'\n",
    "    if not TRAIN_DIR:  TRAIN_DIR  = os.getcwd() + '/train'\n",
    "    %cd $valid_path\n",
    "    for i in xrange(10):\n",
    "        %cd c\"$i\"\n",
    "        g = glob('*.jpg')\n",
    "        for n in xrange(len(g)):\n",
    "            os.rename(g[n], TRAIN_DIR + '/c' + str(i) + '/' + g[n])\n",
    "            counter += 1\n",
    "        % cd ..\n",
    "    if verbose: print(\"Moved {} files.\".format(counter))\n",
    "#         %mv $VALID_DIR/c\"$i\"/$*.jpg $TRAIN_DIR/c\"$i\"/$*.jpg\n",
    "\n",
    "# modified from: http://forums.fast.ai/t/statefarm-kaggle-comp/183/20\n",
    "def set_valid(number=1, verbose=1, data_path=''):\n",
    "    \"\"\"Moves <number> of subjects from training to validation \n",
    "    directories. Verbosity: 0: Silent; 1: print no. files moved; \n",
    "    2: print each move operation\"\"\"\n",
    "    if not data_path: data_path = os.getcwd() + '/'\n",
    "    counter = 0\n",
    "    if number < 0: number = 0\n",
    "    for n in xrange(number):\n",
    "        # read CSV file into Pandas DataFrame\n",
    "        dil = pd.read_csv(data_path + 'driver_imgs_list.csv')\n",
    "        # group frame by subject in image\n",
    "        grouped_subjects = dil.groupby('subject')\n",
    "        # pick <number> subjects at random\n",
    "        subject = grouped_subjects.groups.keys()[np.random.randint(0, high=len(grouped_subjects.groups))] # <-- groups?\n",
    "        # get the group assoc w/ subject\n",
    "        group = grouped_subjects.get_group(subject)\n",
    "        # loop over gropu & move imgs to validation dir\n",
    "        for (subject, clssnm, img) in group.values:\n",
    "            source = '{}train/{}/{}'.format(data_path, clssnm, img)\n",
    "            target = source.replace('train', 'valid')\n",
    "            if verbose > 1: print('mv {} {}'.format(source, target))\n",
    "            os.rename(source, target)\n",
    "            counter += 1\n",
    "    if verbose: print (\"Files moved: {}\".format(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2/3: Making sure we're in the right dir, & moving stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Deshar/Kaukasos/FAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c0\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c1\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c2\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c3\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c4\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c5\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c6\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c7\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c8\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid/c9\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n",
      "Moved 0 files.\n",
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm\n",
      "Files moved: 1894\n"
     ]
    }
   ],
   "source": [
    "%cd data/statefarm/\n",
    "reset_valid()\n",
    "%cd ..\n",
    "set_valid(number=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3/3: copying val set from the full valid folder to sample valid\n",
    "\n",
    "J.Howard uses a permutation of 1,000 val imgs, so I'll just do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/WayNoxchi/Deshar/Kaukasos/FAI/data/statefarm/valid\n"
     ]
    }
   ],
   "source": [
    "%cd valid\n",
    "# g = glob('valid/c?/*.jpg') # <-- this doesnt work: why?\n",
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "# for i in range(1000): copyfile(shuf[i], '/sample/' + shuf[i])\n",
    "for i in range(1000): copyfile(shuf[i], '../sample/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path + 'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path + 'valid', batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames, \n",
    "    test_filename) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Models\n",
    "\n",
    "#### Linear Model\n",
    "\n",
    "First, we try the simplest model and use default parameters. Note the trick of making the first layer a batchnorm layer - that way we don't have to worry about normalizing the input ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "            Flatten(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, this training is going nowhere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 62s - loss: 13.9601 - acc: 0.0987 - val_loss: 14.0630 - val_acc: 0.1270\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 48s - loss: 14.5815 - acc: 0.0953 - val_loss: 14.1033 - val_acc: 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1157954d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches,\n",
    "                   nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the number of parameters to see that there's enough parameters to find some useful relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 224, 224)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 150528)        0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            1505290     flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,505,302\n",
      "Trainable params: 1,505,296\n",
      "Non-trainable params: 6\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1505280"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10*3*224*224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a simple model with no regularization and plenty of parameters, it seems most likely that our learning rate is too hgh. Perhaps it is jumping to a solution where it predicts one or two classes with high confidence, so that it can give a zero prediction to as many classes as possible - that's the best approach for a model that is no better than random, and there is likely to be where we would end up with a high learning rate. So let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict_generator(batches, batches.n)[:10],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = model.predict_generator(batches, batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis was correct. It's nearly always predicting class 1 or 6, with very high confidence. So let's try a lower learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010000000475\n"
     ]
    }
   ],
   "source": [
    "# here's a way to take a look at the learning rate\n",
    "import keras.backend as K\n",
    "LR = K.eval(model.optimizer.lr)\n",
    "print(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 60s - loss: 2.3591 - acc: 0.2133 - val_loss: 5.1346 - val_acc: 0.1070\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 48s - loss: 1.7576 - acc: 0.4280 - val_loss: 3.3038 - val_acc: 0.1950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11929f050>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Flatten(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - we found our way out of that hole ... Now we can increase the learning rate and see where we can get to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 62s - loss: 1.3820 - acc: 0.6053 - val_loss: 2.5085 - val_acc: 0.2770\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 53s - loss: 1.1442 - acc: 0.7033 - val_loss: 2.2965 - val_acc: 0.3320\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 51s - loss: 0.9622 - acc: 0.7793 - val_loss: 1.8484 - val_acc: 0.4600\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 49s - loss: 0.8183 - acc: 0.8200 - val_loss: 1.5792 - val_acc: 0.5460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d92b8d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're stabilizing at validation accuracy of 0.39 *(~.50 in my NB)*. Not great, but a lot better than random. Before moving on, let's check that our validation set on the sample is large enough that it gives consistent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "rnd_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_res = [model.evaluate_generator(rnd_batches, rnd_batches.nb_sample) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.58,  0.55],\n",
       "       [ 1.6 ,  0.54],\n",
       "       [ 1.6 ,  0.54],\n",
       "       [ 1.57,  0.55],\n",
       "       [ 1.55,  0.55],\n",
       "       [ 1.59,  0.54],\n",
       "       [ 1.57,  0.55],\n",
       "       [ 1.57,  0.56],\n",
       "       [ 1.58,  0.55],\n",
       "       [ 1.57,  0.56]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(val_res,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, pretty consistent - if we see imporvements of 3% or more, it's probably not random, based on the above samples.\n",
    "\n",
    "#### L2 Regularization\n",
    "\n",
    "The previous model is over-fitting a lot, but we can't use dropout since we only have one layer. We can try to decrease overfitting in our model by adding [l2 regularization](http://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html/2) (ie: add the sum of squares of the weights to our loss function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 64s - loss: 2.7178 - acc: 0.1647 - val_loss: 5.4327 - val_acc: 0.1270\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 51s - loss: 2.0071 - acc: 0.3827 - val_loss: 4.0393 - val_acc: 0.1990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e75cc10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Flatten(),\n",
    "            Dense(10, activation='softmax', W_regularizer=l2(0.01))\n",
    "        ])\n",
    "model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 62s - loss: 1.6172 - acc: 0.5773 - val_loss: 3.1532 - val_acc: 0.2590\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 48s - loss: 1.3835 - acc: 0.6647 - val_loss: 2.3724 - val_acc: 0.3750\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 49s - loss: 1.1934 - acc: 0.7540 - val_loss: 2.1825 - val_acc: 0.3740\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 48s - loss: 1.0514 - acc: 0.8080 - val_loss: 2.0198 - val_acc: 0.4530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e9cff10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=0.001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we can get a bit over 50% *(almost, here: 45.3%)* accuracy this way. This'll be a good benchmark for our future models - if we can't beat 50%, then we're not even beating a linear model trained on a sample, so we'll know that's not a good approach.\n",
    "\n",
    "### Single hidden layer\n",
    "\n",
    "The next simplest model is to add a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 67s - loss: 1.9732 - acc: 0.3693 - val_loss: 7.3742 - val_acc: 0.1060\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 53s - loss: 1.0514 - acc: 0.7080 - val_loss: 3.8568 - val_acc: 0.2200\n",
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 66s - loss: 0.5922 - acc: 0.8827 - val_loss: 1.8494 - val_acc: 0.4520\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 55s - loss: 0.4276 - acc: 0.9293 - val_loss: 1.3124 - val_acc: 0.5780\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 54s - loss: 0.2759 - acc: 0.9700 - val_loss: 0.9763 - val_acc: 0.7210\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 56s - loss: 0.1899 - acc: 0.9853 - val_loss: 0.9103 - val_acc: 0.7720\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 52s - loss: 0.1516 - acc: 0.9920 - val_loss: 0.9023 - val_acc: 0.7710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116cbdd50>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "            Flatten(),\n",
    "            Dense(100, activation='relu'), #¿would λ2 be good here?\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Odd, I may not have a good validation set if I'm getting such higher valacc numbers...)*\n",
    "\n",
    "Not looking very encouraging... which isn't surprising since we know that CNNs are a much better choice for computer vision problems. So we'll try one.\n",
    "\n",
    "### Single Conv Layer\n",
    "\n",
    "2 conv layers with max pooling followed by a simple dense network is a good simple CNN to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "                BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "                Convolution2D(32, 3, 3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3, 3)),\n",
    "                Convolution2D(64, 3, 3, activation='relu'),\n",
    "                BatchNormalization(axis=1),\n",
    "                MaxPooling2D((3,3)),\n",
    "                Flatten(),\n",
    "                Dense(200, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dense(10, activation='softmax')\n",
    "            ])\n",
    "    model.compile(Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches,\n",
    "                        nb_val_samples=val_batches.nb_sample)\n",
    "    \n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches,\n",
    "                        nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 433s - loss: 1.8576 - acc: 0.4900 - val_loss: 4.1450 - val_acc: 0.1160\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 410s - loss: 0.5595 - acc: 0.8873 - val_loss: 2.4079 - val_acc: 0.2540\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 436s - loss: 0.1618 - acc: 0.9820 - val_loss: 2.4342 - val_acc: 0.2540\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 430s - loss: 0.0788 - acc: 0.9913 - val_loss: 2.8377 - val_acc: 0.1370\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 418s - loss: 0.0269 - acc: 1.0000 - val_loss: 2.9813 - val_acc: 0.1300\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 426s - loss: 0.0112 - acc: 1.0000 - val_loss: 2.8279 - val_acc: 0.1840\n"
     ]
    }
   ],
   "source": [
    "conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set here is very rapidly reaching a very high accuracy. So if we could regularize this, perhaps we could get a reasonable results.\n",
    "\n",
    "So, what kind of regularization should we try first? As we discussed in lesson 3, we should start with data augmentation.\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "To find the best data augmentation parameters, we can try each type of data augmentation, one at a time. For each type, we can try four very different levels of augmentation, and see which is the best. In the steps below we've only kept the single best results we found. We're using the CNN we defined above, since we have already observed it can model the data quickly and accurately.\n",
    "\n",
    "Width shift: move the image left and right -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "batches = get_batches(path + 'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 441s - loss: 2.1688 - acc: 0.3453 - val_loss: 5.9133 - val_acc: 0.1050\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 431s - loss: 1.2125 - acc: 0.6373 - val_loss: 2.6613 - val_acc: 0.1700\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 417s - loss: 0.7882 - acc: 0.7660 - val_loss: 2.4997 - val_acc: 0.1930\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 437s - loss: 0.5987 - acc: 0.8267 - val_loss: 2.6768 - val_acc: 0.1200\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 439s - loss: 0.4185 - acc: 0.8820 - val_loss: 3.1566 - val_acc: 0.1800\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 453s - loss: 0.3207 - acc: 0.9147 - val_loss: 3.4434 - val_acc: 0.1820\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Height shift: move the image up and down - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "batches = get_batches(path + 'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random shear angles (max in radians) - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(shear_range=0.1)\n",
    "batches = get_batches(path + 'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotation: max in degrees - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15)\n",
    "batches = get_batches(path + 'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel shift: randomly changing the R,B,G colors - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(channel_shift_range=20)\n",
    "batches = get_batches(path + 'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path + 'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this isn't looking encouraging, since the validation set is poor and getting worse. But the training set is getting better, and still has a long way to go in accuracy - so we should try annealing our learning rate and running more epochs, before we make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches,\n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucky we tried that - we're starting to make progress! Let's keep going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=25, validation_data=val_batches,\n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, using nothing but a small sample, a simple (not pre-trianed) model with no dropout, and data augmentation, we're getting results that would get us into the top 50% of the competition! This looks like a great foundation for our further experiments.\n",
    "\n",
    "To go further, we'll need to use the whole dataset, since dropout and data volumes are very related, so we can't tweak dropout without using all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
