{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Wayne Nixalo  -  20 May 2017\n",
    "FAI1 - Practical Deep Learning I - Week 3 HW: Kaggle StateFarm Distracted Driver Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/wnixalo/miniconda3/envs/FAI/lib/python2.7/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 870M (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will need this to access any libraries in superdirectories\n",
    "sys.path.insert(1, os.path.join(os.getcwd(), '../utils'))\n",
    "import utils\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Run this the First Time Only\n",
    "\n",
    "Download the Data and get it into the right directories. Fortunately SF already organized the data, so it's just a matter of assigning path variables. NOTE: kaggle-cli needs to be set up beforhand. Also path vars must be assigned each time this notebook is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR = os.getcwd()\n",
    "DATA_DIR = HOME_DIR + '/data'\n",
    "TRAIN_DIR = DATA_DIR + '/train'\n",
    "VAL_DIR = DATA_DIR + '/valid'\n",
    "TEST_DIR = DATA_DIR + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the validation directories\n",
    "# os.mkdir(VAL_PATH)\n",
    "# for i in xrange(10):\n",
    "#     os.mkdir(VAL_PATH + '/c' + str(i))\n",
    "\n",
    "# # another way to do this:\n",
    "# %mkdir $VAL_PATH\n",
    "# for i in xrange(10):\n",
    "#     %mkdir $VAL_PATH/c\"$i\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run this if you don't have an Accurate Validation Set\n",
    "\n",
    "Grab a random permutation from the training data for validation. Do this until validation accuracy matches test accuracy. Also see: http://stackoverflow.com/questions/2632205/how-to-count-the-number-of-files-in-a-directory-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %cd $TRAIN_PATH\n",
    "\n",
    "# VAL_PORTION = 0.2\n",
    "# for i in xrange(10):\n",
    "#     %cd c\"$i\"\n",
    "#     g = glob('*.jpg')\n",
    "#     number = len(g)\n",
    "#     shuff = np.random.permutation(g)\n",
    "#     for n in xrange(int(number * VAL_PORTION)):\n",
    "#         os.rename(shuff[n], VAL_DIR + '/c' + str(i) + '/' + shuff[n])\n",
    "#     % cd ..\n",
    "\n",
    "def reset_valid(verbose=1):\n",
    "    \"\"\"Moves all images in validation set back to \n",
    "    their respective classes in the training set.\"\"\"\n",
    "    counter = 0\n",
    "    %cd $valid_path\n",
    "    for i in xrange(10):\n",
    "        %cd c\"$i\"\n",
    "        g = glob('*.jpg')\n",
    "        for n in xrange(len(g)):\n",
    "            os.rename(g[n], TRAIN_DIR + '/c' + str(i) + '/' + g[n])\n",
    "            counter += 1\n",
    "        % cd ..\n",
    "    if verbose: print(\"Moved {} files.\".format(counter))\n",
    "#         %mv $VALID_DIR/c\"$i\"/$*.jpg $TRAIN_DIR/c\"$i\"/$*.jpg\n",
    "\n",
    "# modified from: http://forums.fast.ai/t/statefarm-kaggle-comp/183/20\n",
    "def set_valid(number=1, verbose=1):\n",
    "    \"\"\"Moves <number> of subjects from training to validation \n",
    "    directories. Verbosity: 0: Silent; 1: print no. files moved; \n",
    "    2: print each move operation\"\"\"\n",
    "    counter = 0\n",
    "    if number < 0: number = 0\n",
    "    for n in xrange(number):\n",
    "        # read CSV file into Pandas DataFrame\n",
    "        dil = pd.read_csv(data_path + 'driver_imgs_list.csv')\n",
    "        # group frame by subject in image\n",
    "        grouped_subjects = dil.groupby('subject')\n",
    "        # pick <number> subjects at random\n",
    "        subject = grouped_subjects.groups.keys()[np.random.randint(0, high=len(grouped_subjects.groups))] # <-- groups?\n",
    "        # get the group assoc w/ subject\n",
    "        group = grouped_subjects.get_group(subject)\n",
    "        # loop over gropu & move imgs to validation dir\n",
    "        for (subject, clssnm, img) in group.values:\n",
    "            source = '{}train/{}/{}'.format(data_path, clssnm, img)\n",
    "            target = source.replace('train', 'valid')\n",
    "            if verbose > 1: print('mv {} {}'.format(source, target))\n",
    "            os.rename(source, target)\n",
    "            counter += 1\n",
    "    if verbose: print \"Files moved: {}\".format(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dil = pd.read_csv(data_path + 'driver_imgs_list.csv')\n",
    "# grouped_subjects = dil.groupby('subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'subject', u'classname', u'img'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dil.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(grouped_subjects.groups) # <-- that's what I'm looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c0\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c1\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c2\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c3\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c4\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c5\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c6\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c7\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c8\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid/c9\n",
      "/home/wnixalo/Kaukasos/FAI/lesson3/data/valid\n",
      "Moved 2110 files.\n"
     ]
    }
   ],
   "source": [
    "reset_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moved: 3039\n"
     ]
    }
   ],
   "source": [
    "set_valid(number=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some more setup\n",
    "data_path    = DATA_DIR  + '/'\n",
    "train_path   = TRAIN_DIR + '/'\n",
    "valid_path   = VAL_DIR   + '/'\n",
    "test_path    = TEST_DIR  + '/'\n",
    "results_path = DATA_DIR  + '/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# looks like batch size of 64 is just past what my GPU can handle\n",
    "# would using bcolz to save precomputed arrays help?\n",
    "batch_size=32\n",
    "target_size=(224,224) # for gen.flow_from_directory(..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19385 images belonging to 10 classes.\n",
      "Found 3039 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# batch generator to feed data into the model\n",
    "gen = image.ImageDataGenerator()\n",
    "trn_batches = gen.flow_from_directory(train_path, target_size=target_size,\n",
    "                class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "val_batches = gen.flow_from_directory(valid_path, target_size=target_size,\n",
    "                class_mode='categorical', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19385"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_batches.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I'll want a way to clear GPU memory in the future. Right now all I know is restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the VGG model, download its weights, and finetune it to the data\n",
    "VGG = Vgg16()\n",
    "VGG.model.pop()\n",
    "for layer in VGG.model.layers: layer.trainable = False\n",
    "VGG.model.add(Dense(10, activation='softmax'))\n",
    "VGG.model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19385/19385 [==============================] - 1096s - loss: 1.7957 - acc: 0.4986 - val_loss: 1.4063 - val_acc: 0.5745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08d5573a50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the model until it overfits\n",
    "VGG.model.optimizer.lr = 0.001\n",
    "VGG.model.fit_generator(trn_batches, trn_batches.n, nb_epoch=1, verbose=1,\n",
    "                       validation_data=val_batches, nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(lr=0.001, epochs=1, verbose=0):\n",
    "    VGG.model.optimizer.lr=lr\n",
    "    VGG.model.fit_generator(trn_batches, trn_batches.n, nb_epoch=epochs, verbose=verbose,\n",
    "                           validation_data=val_batches, nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "19385/19385 [==============================] - 1096s - loss: 1.0948 - acc: 0.6601 - val_loss: 1.4272 - val_acc: 0.5926\n",
      "Epoch 2/2\n",
      "19385/19385 [==============================] - 1096s - loss: 0.9876 - acc: 0.7021 - val_loss: 1.6798 - val_acc: 0.4656\n"
     ]
    }
   ],
   "source": [
    "train_model(lr=0.1, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG.model.save_weights(data_path + 'finetune01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "19385/19385 [==============================] - 1096s - loss: 1.0026 - acc: 0.7094 - val_loss: 1.2893 - val_acc: 0.6147\n",
      "Epoch 2/2\n",
      "19385/19385 [==============================] - 1095s - loss: 1.0080 - acc: 0.7159 - val_loss: 1.3807 - val_acc: 0.5683\n",
      "Epoch 1/2\n",
      "19385/19385 [==============================] - 1092s - loss: 0.9848 - acc: 0.7269 - val_loss: 1.1980 - val_acc: 0.6081\n",
      "Epoch 2/2\n",
      "19385/19385 [==============================] - 1093s - loss: 1.0269 - acc: 0.7290 - val_loss: 1.1896 - val_acc: 0.6190\n"
     ]
    }
   ],
   "source": [
    "train_model(lr=0.01, epochs=2, verbose=1)\n",
    "VGG.model.save_weights(data_path + 'finetune02.h5')\n",
    "train_model(lr=0.001, epochs=2, verbose=1)\n",
    "VGG.model.save_weights(data_path + 'finetune03.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "19385/19385 [==============================] - 1095s - loss: 0.9968 - acc: 0.7379 - val_loss: 1.2430 - val_acc: 0.6242\n",
      "Epoch 2/4\n",
      "19385/19385 [==============================] - 1096s - loss: 0.9956 - acc: 0.7378 - val_loss: 1.2325 - val_acc: 0.6252\n",
      "Epoch 3/4\n",
      "19385/19385 [==============================] - 1092s - loss: 1.0270 - acc: 0.7388 - val_loss: 1.7015 - val_acc: 0.5318\n",
      "Epoch 4/4\n",
      "19385/19385 [==============================] - 1092s - loss: 1.0012 - acc: 0.7448 - val_loss: 1.5951 - val_acc: 0.5670\n"
     ]
    }
   ],
   "source": [
    "train_model(lr=0.0001, epochs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving weights\n",
    "# VGG.model.save_weights(data_path + 'finetune01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG.model.load_weights(data_path + 'finetune03.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical',\n",
    "                target_size=(224,224)):\n",
    "    return gen.flow_from_directory(dirname, target_size=target_size,\n",
    "            class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "def get_data(path, target_size=(224,224)):\n",
    "    batches = get_batches(path, shuffle=False, batch_size=1, class_mode=None, target_size=target_size)\n",
    "    return np.concatenate([batches.next() for i in xrange(batches.nb_sample)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# output test data predictions\n",
    "gen = image.ImageDataGenerator()\n",
    "tst_batches = gen.flow_from_directory(test_path, target_size=target_size,\n",
    "                class_mode='categorical', shuffle=False, batch_size=batch_size*2)\n",
    "# predictions = VGG.model.predict_on_batch(tst_batches)\n",
    "# predictions = VGG.model.predict(tst_batches, batch_size=batch_size*2, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79726"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_batches.n\n",
    "tst_batches.nb_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```enumerate``` is zero-indexed. The first Conv layer is the 3rd, so that'll give an index of 2. Therefore, when defining a model consisting of all Conv layers, remember to include 1-past the index. Also, Python index notation ```[x:y]``` translates to ```[start-before-here:until-just-before-here]``` equivalently, ```[start-with-this:until-this]```\n",
    "\n",
    "So that's why, if your model is ```VGG.model```, entering ```VGG.model.layers[last_conv_idx]``` will give you the last convolutional layer, but entering: ```VGG.model.layers[:last_conv_idx]``` will give you all layers *up to* the last Conv layer, but *not including it*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Convolution2D\n",
    "last_conv_idx = [index for index, layer in enumerate(VGG.model.layers) if type(layer) is Convolution2D][-1]\n",
    "conv_layers = VGG.model.layers[:last_conv_idx + 1]\n",
    "# NOTE: enumerate is zero-indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_conv_idx = [idx for idx, layer in enumerate(VGG.model.layers) if type(layer) is Convolution2D][0]\n",
    "first_conv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Lambda at 0x7f08d8b4dad0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d8035550>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d800a3d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7f6fc50>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7f7c990>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d8035510>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7f46e10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7eeb750>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7ef2790>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7f089d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7f46dd0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7ef23d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7eab690>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7eb8650>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7eb8a90>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7ed5950>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e6be50>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7ef21d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7e77650>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e8f890>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7e8f450>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e2dd50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7e38d10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e53250>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7e77610>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7dd97d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7df0c50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7dfe150>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e12150>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7da0110>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7daf610>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7dd9790>,\n",
       " <keras.layers.core.Flatten at 0x7f08d7dbbad0>,\n",
       " <keras.layers.core.Dense at 0x7f08d7d50f50>,\n",
       " <keras.layers.core.Dropout at 0x7f088e4e0ad0>,\n",
       " <keras.layers.core.Dense at 0x7f08d7a1d410>,\n",
       " <keras.layers.core.Dropout at 0x7f08d7a27f50>,\n",
       " <keras.layers.core.Dense at 0x7f08d8b4db50>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(last_conv_idx)\n",
    "VGG.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Lambda at 0x7f08d8b4dad0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d8035550>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d800a3d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7f6fc50>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7f7c990>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d8035510>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7f46e10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7eeb750>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7ef2790>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7f089d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7f46dd0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7ef23d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7eab690>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7eb8650>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7eb8a90>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7ed5950>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e6be50>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7ef21d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7e77650>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e8f890>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7e8f450>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e2dd50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7e38d10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e53250>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7e77610>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7dd97d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7df0c50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7dfe150>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e12150>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7da0110>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7daf610>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Lambda at 0x7f08d8b4dad0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d8035550>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d800a3d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7f6fc50>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7f7c990>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d8035510>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7f46e10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7eeb750>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7ef2790>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7f089d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7f46dd0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7ef23d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7eab690>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7eb8650>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7eb8a90>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7ed5950>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e6be50>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7ef21d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7e77650>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e8f890>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7e8f450>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e2dd50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7e38d10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e53250>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f08d7e77610>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7dd97d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7df0c50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7dfe150>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f08d7e12150>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f08d7da0110>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.layers.pooling import MaxPooling2D\n",
    "# ??MaxPooling2D\n",
    "# ??Convolution2D\n",
    "# conv_layers[-1].output_shape\n",
    "# VGG.model.layers[last_conv_idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512, 14, 14)\n",
      "(None, 512, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "# getting some insight into Conv output_shape and MaxPool2D input_shapes\n",
    "# Theano ordering: (samples, channels, rows, cols) <- Conv-out / MxP-in\n",
    "# \n",
    "print(VGG.model.layers[last_conv_idx].output_shape)\n",
    "print(VGG.model.layers[last_conv_idx + 1].input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I don't know if, when building an FC model taking input from a Vgg16bn Conv model, I'll have to specifiy input shape to the first MaxPooling2D layer to be all but the first part of the output_shape (tensor?) from the ConvLayer. We'll see. Does it matter because it's ```None```? or will it be inferred automatically? Or does it need to take in a 3-dim input -- but then why would it be taking in a 4-dim one in the full model's case? Hmm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "predictions = VGG.test(test_path, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(results_path + 'raw_predictions01.bc', predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79726"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79726, 10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = predictions[1]\n",
    "# clipping & renorm to score better on logloss metric\n",
    "preds = utils.do_clip(preds, mx=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = tst_batches.filenames\n",
    "# ids = np.array([str(f[8:f.find('.')]) for f in filenames])\n",
    "ids = np.array([str(f[8:]) for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79726,)\n",
      "(79726, 10)\n"
     ]
    }
   ],
   "source": [
    "print(ids.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submissions = np.stack([ids, preds], axis=1)\n",
    "\n",
    "# couldn't get the older method of using np.stack to work, so trying pandas\n",
    "classes = sorted(trn_batches.class_indices, key=trn_batches.class_indices.get)\n",
    "submission = pd.DataFrame(preds, columns=classes)\n",
    "# submission.insert(0, 'img', [f[12:] for f in filenames])\n",
    "submission.insert(0, 'img', [f[8:] for f in filenames])\n",
    "submission.head()\n",
    "submission.to_csv(results_path + 'submission.csv', index=False, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ??pd.DataFrame.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/wnixalo/Kaukasos/FAI/lesson3/data/results/submission.csv' target='_blank'>/home/wnixalo/Kaukasos/FAI/lesson3/data/results/submission.csv</a><br>"
      ],
      "text/plain": [
       "/home/wnixalo/Kaukasos/FAI/lesson3/data/results/submission.csv"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(results_path + 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `submission.insert` not found.\n"
     ]
    }
   ],
   "source": [
    "??submission.insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Results: place|score: ```658/1440|1.50925```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
