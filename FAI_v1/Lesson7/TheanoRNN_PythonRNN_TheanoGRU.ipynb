{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wayne Nixalo - 2 Jul 2017\n",
    "\n",
    "    Theano RNN / Python RNN / Theano GRU\n",
    "    \n",
    "    (from L6 & L7 JNBs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# I. Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import os, sys\n",
    "sys.path.insert(1, os.path.join('../utils'))\n",
    "from utils import *\n",
    "# %matplotlib inline\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600901\n",
      "total chars: 86\n"
     ]
    }
   ],
   "source": [
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars) + 1\n",
    "print('total chars:', vocab_size)\n",
    "\n",
    "chars.insert(0, \"\\0\")\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75110, 8, 86), (75110, 8, 86))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = 8\n",
    "\n",
    "c_in_dat = [[idx[i+n] for i in xrange(0, len(idx)-1-cs, cs)] for n in xrange(cs)]\n",
    "xs = [np.stack(c[:-2]) for c in c_in_dat]\n",
    "\n",
    "c_out_dat = [[idx[i+n] for i in xrange(1, len(idx)-cs, cs)] for n in xrange(cs)]\n",
    "ys = [np.stack(c[:-2]) for c in c_out_dat]\n",
    "\n",
    "oh_ys = [to_categorical(o, vocab_size) for o in ys]\n",
    "oh_y_rnn = np.stack(oh_ys, axis=1)\n",
    "\n",
    "oh_xs = [to_categorical(o, vocab_size) for o in xs]\n",
    "oh_x_rnn = np.stack(oh_xs, axis=1)\n",
    "\n",
    "oh_x_rnn.shape, oh_y_rnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# II. Theano RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 25.173\n",
      "Error: 21.511\n",
      "Error: 20.921\n",
      "Error: 19.932\n",
      "Error: 18.820\n",
      "Error: 19.238\n",
      "Error: 19.018\n",
      "Error: 18.393\n",
      "Error: 17.890\n",
      "Error: 18.225\n",
      "Error: 17.423\n",
      "Error: 17.590\n",
      "Error: 18.349\n",
      "Error: 17.232\n",
      "Error: 16.761\n",
      "Error: 17.729\n",
      "Error: 17.354\n",
      "Error: 17.192\n",
      "Error: 16.764\n",
      "Error: 16.657\n",
      "Error: 16.539\n",
      "Error: 16.380\n",
      "Error: 16.632\n",
      "Error: 16.148\n",
      "Error: 16.789\n",
      "Error: 16.560\n",
      "Error: 16.056\n",
      "Error: 16.306\n",
      "Error: 16.293\n",
      "Error: 16.475\n",
      "Error: 16.688\n",
      "Error: 16.318\n",
      "Error: 16.640\n",
      "Error: 16.292\n",
      "Error: 16.009\n",
      "Error: 16.652\n",
      "Error: 15.910\n",
      "Error: 16.352\n",
      "Error: 15.995\n",
      "Error: 16.256\n",
      "Error: 15.300\n",
      "Error: 15.650\n",
      "Error: 15.773\n",
      "Error: 15.940\n",
      "Error: 15.972\n",
      "Error: 15.850\n",
      "Error: 15.628\n",
      "Error: 16.097\n",
      "Error: 15.940\n",
      "Error: 16.046\n",
      "Error: 15.177\n",
      "Error: 15.636\n",
      "Error: 14.939\n",
      "Error: 14.891\n",
      "Error: 15.644\n",
      "Error: 15.284\n",
      "Error: 14.673\n",
      "Error: 15.461\n",
      "Error: 15.125\n",
      "Error: 14.967\n",
      "Error: 14.999\n",
      "Error: 15.380\n",
      "Error: 15.313\n",
      "Error: 15.080\n",
      "Error: 14.684\n",
      "Error: 14.812\n",
      "Error: 14.258\n",
      "Error: 14.758\n",
      "Error: 15.191\n",
      "Error: 14.766\n",
      "Error: 15.128\n",
      "Error: 14.651\n",
      "Error: 14.432\n",
      "Error: 14.430\n",
      "Error: 14.484\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 256; n_Fac = 42; cs = 8\n",
    "\n",
    "n_input = vocab_size; n_output = vocab_size\n",
    "\n",
    "def init_wgts(rows, cols):\n",
    "    scale = math.sqrt(2 / rows)\n",
    "    return shared(normal(scale=scale, size=(rows,cols)).astype(np.float32))\n",
    "def init_bias(rows):\n",
    "    return shared(np.zeros(rows, dtype=np.float32))\n",
    "def wgts_and_bias(n_in, n_out):\n",
    "    return init_wgts(n_in, n_out), init_bias(n_out)\n",
    "def id_and_bias(n):\n",
    "    return shared(np.eye(n, dtype=np.float32)), init_bias(n)\n",
    "\n",
    "# Theano Variables\n",
    "t_inp = T.matrix('inp')\n",
    "t_outp = T.matrix('outp')\n",
    "t_h0 = T.vector('h0')\n",
    "lr = T.scalar('lr')\n",
    "\n",
    "all_args = [t_h0, t_inp, t_outp, lr]\n",
    "\n",
    "W_h = id_and_bias(n_hidden)\n",
    "W_x = wgts_and_bias(n_input, n_hidden)\n",
    "W_y = wgts_and_bias(n_hidden, n_output)\n",
    "w_all = list(chain.from_iterable([W_h, W_x, W_y]))\n",
    "\n",
    "def step(x, h, W_h, b_h, W_x, b_x, W_y, b_y):\n",
    "    # Calc hidden activations\n",
    "    h = nnet.relu(T.dot(x, W_x) + b_x + T.dot(h, W_h) + b_h)\n",
    "    # Calc output activations\n",
    "    y = nnet.softmax(T.dot(h, W_y) + b_y)\n",
    "    # Return both -- `flatten()` is workaround: Theano bug\n",
    "    return h, T.flatten(y, 1)\n",
    "\n",
    "[v_h, v_y], _ = theano.scan(step, sequences=t_inp,\n",
    "                           outputs_info=[t_h0, None], non_sequences=w_all)\n",
    "error = nnet.categorical_crossentropy(v_y, t_outp).sum()\n",
    "g_all = T.grad(error, w_all)\n",
    "\n",
    "def upd_dict(wgts, grads, lr):\n",
    "    return OrderedDict({w: w - g*lr for (w,g) in zip(wgts, grads)})\n",
    "\n",
    "upd = upd_dict(w_all, g_all, lr)\n",
    "\n",
    "# ready to compile function\n",
    "fn = theano.function(all_args, error, updates=upd, allow_input_downcast=True)\n",
    "\n",
    "X = oh_x_rnn\n",
    "Y = oh_y_rnn\n",
    "\n",
    "# semi-auto SGD loop:\n",
    "err = 0.0; l_rate = 0.01\n",
    "for i in xrange(len(X)):\n",
    "    err += fn(np.zeros(n_hidden), X[i], Y[i], l_rate)\n",
    "    if i % 1000 == 999:\n",
    "        print (\"Error: {:.3f}\".format(err/1000))\n",
    "        err=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key step done automatically above by Theano is gradients calculation.\n",
    "\n",
    "---\n",
    "# III. Python RNN\n",
    "\n",
    "Will need to define all functions used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_d(x):\n",
    "    output = sigmoid(x)\n",
    "    return output * (1 - output)\n",
    "\n",
    "def relu(x): return np.maximum(0., x)\n",
    "def relu_d(x): return (x > 0.) * 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  0.]), array([ 1.,  0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu fn test\n",
    "relu(np.array([3.,-3.])), relu_d(np.array([3.,-3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "def dist(a, b): return pow(a - b, 2)\n",
    "def dist_d(a, b): return 2 * (a - b)\n",
    "\n",
    "import pdb\n",
    "\n",
    "# CrossEntropy\n",
    "eps = 1e-7\n",
    "def x_entropy(pred, actual):\n",
    "    return -np.sum(actual * np.log(np.clip(pred, eps, 1 - eps))) # clipping for infs\n",
    "def x_entropy_d(pred, actual): return -actual / pred\n",
    "\n",
    "def softmax(x): return np.exp(x) / np.exp(x).sum()\n",
    "def softmax_d(x):\n",
    "    sm = softmax(x)\n",
    "    res = np.expand_dims(-sm, -1) * sm\n",
    "    res[np.diag_indices_from(res)] = sm * (1 - sm)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tests: Check manual answers same as Theano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.35667494393873245)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = np.array([0.2, 0.7, 0.1])\n",
    "test_actuals = np.array([0.,1.,0.])\n",
    "# Theano\n",
    "nnet.categorical_crossentropy(test_preds, test_actuals).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35667494393873245"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual\n",
    "x_entropy(test_preds, test_actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.    , -1.4286, -0.    ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp = T.dvector()\n",
    "test_out = nnet.categorical_crossentropy(test_inp, test_actuals)\n",
    "test_grad = theano.function([test_inp], T.grad(test_out, test_inp))\n",
    "# Theano\n",
    "test_grad(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.    , -1.4286, -0.    ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual\n",
    "x_entropy_d(test_preds, test_actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = x_entropy\n",
    "loss_d = x_entropy_d\n",
    "act = relu\n",
    "act_d = relu_d\n",
    "\n",
    "pre_pred = random(oh_x_rnn[0][0].shape)\n",
    "preds = softmax(pre_pred)\n",
    "actual = oh_x_rnn[0][0]\n",
    "\n",
    "np.allclose(softmax_d(pre_pred).dot(loss_d(preds, actual)), preds - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2814  0.464   0.2546]\n",
      "[[ 0.2814  0.464   0.2546]]\n",
      "\n",
      "[[ 0.2022 -0.1306 -0.0717]\n",
      " [-0.1306  0.2487 -0.1181]\n",
      " [-0.0717 -0.1181  0.1898]]\n",
      "\n",
      "[[ 0.2022 -0.1306 -0.0717]\n",
      " [-0.1306  0.2487 -0.1181]\n",
      " [-0.0717 -0.1181  0.1898]]\n"
     ]
    }
   ],
   "source": [
    "print (softmax(test_preds))\n",
    "print (nnet.softmax(test_preds).eval())\n",
    "\n",
    "print()\n",
    "\n",
    "test_out = T.flatten(nnet.softmax(test_inp))\n",
    "test_grad = theano.function([test_inp], theano.gradient.jacobian(test_out, test_inp))\n",
    "\n",
    "print(test_grad(test_preds))\n",
    "print()\n",
    "print(softmax_d(test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also need to define a scan function. Not concerned w/ running things in parallel here, so it's simpe to implement:\n",
    "\n",
    "`scan(..)`: go thru a sequences, 1 step at a time; calling a function on ea. element of the sequence. Each time, the function gets 2 things: the next elem of the seq as well as the previous result of the call.\n",
    "\n",
    "NOTE: Theano doesn't use a `for-loop`, it creates a tree that simultaneously & gradually combines everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 6, 10]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using relu as activation\n",
    "act = relu\n",
    "act_d = relu_d\n",
    "# using crossentropy as loss function\n",
    "loss = x_entropy\n",
    "loss_d = x_entropy_d\n",
    "\n",
    "def scan(fn, start, seq):\n",
    "    res = []\n",
    "    prev = start\n",
    "    for s in seq:\n",
    "        app = fn(prev, s)\n",
    "        res.append(app)\n",
    "        prev = app\n",
    "    return res\n",
    "\n",
    "# test: scan on add 2 things together, on integers 0..4\n",
    "scan(lambda prev, curr: prev + curr, 0, range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training:\n",
    "\n",
    "Building the functions to do the Forward & Backward passes of our RNN. 1st, define data & shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75110, 8, 86), (75110, 8, 86))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8-char seqs 1Hencoded\n",
    "inp = oh_x_rnn\n",
    "# 8-char seqs 1Henc moved aXs by 1\n",
    "outp = oh_y_rnn\n",
    "\n",
    "n_input = vocab_size; n_output = vocab_size; n_hidden = 256\n",
    "inp.shape, outp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functino to do a single forward pass of an RNN, for a single character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "def one_char(prev, item):\n",
    "    # Previous state:\n",
    "    tot_loss, pre_hidden, pre_pred, hidden, ypred = prev\n",
    "    # Current inputs and output\n",
    "    x, y = item\n",
    "    pre_hidden = np.dot(x, w_x) + np.dot(hidden, w_h)\n",
    "    hidden = act(pre_hidden)\n",
    "    pre_pred = np.dot(hidden, w_y)\n",
    "    ypred = softmax(pre_pred)\n",
    "    return (\n",
    "        # Keep track of loss so we can report it\n",
    "        tot_loss + loss(ypred, y), \n",
    "        # Used in Backprop\n",
    "        pre_hidden, pre_pred, \n",
    "        # Used in next iteration\n",
    "        hidden, \n",
    "        # to provide predictions\n",
    "        ypred)\n",
    "\n",
    "# We use scan to apply the above to a whole sequences of characters:\n",
    "def get_chars(n): return zip(inp[n], outp[n])\n",
    "# scan thru all chars in nth phrase, the input & output, calling some function\n",
    "def one_fwd(n): return scan(one_char, (0,0,0,np.zeros(n_hidden),0), get_chars(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the backward step. We use a loop to go through every element of the sequence. The derivatives are applying the chain rule to each step, and accumulating the gradients across the sequences. In the cell below are all the steps necessary to do BackProp thru an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Columnify\" a vector\n",
    "def col(x): return x[:,newaxis]\n",
    "\n",
    "def one_bkwd(args, n):\n",
    "    global w_x, w_y, w_h\n",
    "    \n",
    "    i = inp[n]  # 8x86    # grab one of our inputs\n",
    "    o = outp[n] # 8x86    # grab one of our outputs\n",
    "    d_pre_hidden = np.zeros(n_hidden) # 256\n",
    "    for p in reversed(range(len(i))):    # and go backwards thru ea of the 8 chars, end->start\n",
    "        totloss, pre_hidden, pre_pred, hidden, ypred = args[p]\n",
    "        x = i[p] # 86\n",
    "        y = o[p] # 86\n",
    "        d_pre_pred = softmax_d(pre_pred).dot(loss_d(ypred, y)) # 86\n",
    "        d_pre_hidden = (np.dot(d_pre_hidden, w_h.T)\n",
    "                        + np.dot(d_pre_pred, w_y.T)) * act_d(pre_hidden) # 256\n",
    "        # <Now we can update our weights>\n",
    "        # d(loss)/d(w_y) = d(loss)/d(pre_pred) * d(pre_pred)/d(w_y)\n",
    "        w_y -= col(hidden) * d_pre_pred * alpha # alpha = l_rate\n",
    "        # d(loss)/d(w_h) = d(loss)/d(pre_hidden[p-1]) * d(pre_hidden[p-1])/d(w_h)\n",
    "        if (p > 0): w_h -= args[p-1][3].dot(d_pre_hidden) * alpha\n",
    "        w_x -= col(x) * d_pre_hidden * alpha\n",
    "    return d_pre_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Derivative of Loss gets us from loss to output triangle. Deriv of softmax gets us from triangle to other side of the activation function (blue arrow in Lecture 7 ~[1:38:59](https://youtu.be/Q0z-l2KRYFY?t=5939)) That's where `d_pre_pred` gets us to.\n",
    "\n",
    ">We want to get to the other side of the hidden matrix. The Deriv of a MatxMul is the Mul w/ the Transp of that Matx.\n",
    "wrt hidden:\n",
    "    \n",
    "        (np.dot(d_pre_hidden, w_h.T)\n",
    "\n",
    ">The hidden (circle) has 2 arrows going in & out: so we have to add those derivs together.\n",
    "wrt outputs:\n",
    "    + np.dot(d_pre_pred, w_y.T))\n",
    "   \n",
    ">Finally need to undo the activation: multiply by deriv of activn fn:\n",
    "    * act_d(pre_hidden)\n",
    "\n",
    ">That's the chain-rule that gets us all the way back to the input-side of the green arrow.\n",
    "\n",
    ">Undo the multiplication by the hidden state to get the derivative wrt the weights; for that the 'Columnify' function does: turning a vector into a column.\n",
    "\n",
    "        w_y -= col(hidden) * d_pre_pred * alpha \n",
    ">which creates the new output weights. The new hidden weights are essentially the same: learning rate * derivative we just calculated & we have to undo its weights..\n",
    "\n",
    "        w_h -= args[p-1][3].dot(d_pre_hidden) * alpha\n",
    "\n",
    ">The new new input weights:\n",
    "\n",
    "        w_x -= col(x) * d_pre_hidden * alpha\n",
    "    \n",
    "    \n",
    "Now we can set up our initial weight matrices. Note that we're not using bias at all in this example iot keep things simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = math.sqrt(2 / n_input)\n",
    "w_x = normal(scale=scale, size=(n_input, n_hidden))\n",
    "w_y = normal(scale=scale, size=(n_hidden, n_output))\n",
    "w_h = np.eye(n_hidden, dtype=np.float32) # I matx to initlz hidden weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop looks much like the Theano loop in the prevs section, except that we have to call the backwards step manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:27.9695; Gradient:3.62059\n",
      "Error:27.3908; Gradient:2.72600\n",
      "Error:27.4396; Gradient:4.00864\n",
      "Error:26.9118; Gradient:3.83728\n",
      "Error:26.4892; Gradient:3.37439\n",
      "Error:26.7820; Gradient:3.03459\n",
      "Error:26.3606; Gradient:4.44414\n",
      "Error:26.1111; Gradient:2.91712\n",
      "Error:25.8341; Gradient:4.14549\n",
      "Error:26.1048; Gradient:3.51951\n"
     ]
    }
   ],
   "source": [
    "overallError = 0\n",
    "alpha = 0.0001\n",
    "for n in range(10000):\n",
    "    # run 1 F Step\n",
    "    res = one_fwd(n)\n",
    "    overallError += res[-1][0]\n",
    "    # run 1 B Step\n",
    "    deriv = one_bkwd(res, n)\n",
    "    # print update\n",
    "    if (n % 1000 == 999):\n",
    "        print (\"Error:{:.4f}; Gradient:{:.5f}\".format(\n",
    "                overallError/1000, np.linalg.norm(deriv)))\n",
    "        overallError = 0\n",
    "\n",
    "# Currently, the Forward Step is passing to scan an initial state of zeros every \n",
    "# update. So it's resetting state on each step. To run statefully: just edit\n",
    "# `one_fwd(n)` to return its final state and keep track of it to feed back into it \n",
    "# the next time thru the loop. This however, will risk exploding Gradients & \n",
    "# Activations unless implemented as an LSTM or GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# IV. Keras GRU\n",
    "\n",
    "Gated Recurrent Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WayNoxchi/Miniconda3/Theano/theano/tensor/basic.py:5130: UserWarning: flatten outdim parameter is deprecated, use ndim instead.\n",
      "  \"flatten outdim parameter is deprecated, use ndim instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "75110/75110 [==============================] - 45s - loss: 2.3730    \n",
      "Epoch 2/8\n",
      "75110/75110 [==============================] - 42s - loss: 1.9689    \n",
      "Epoch 3/8\n",
      "75110/75110 [==============================] - 45s - loss: 1.8647    \n",
      "Epoch 4/8\n",
      "75110/75110 [==============================] - 43s - loss: 1.8049    \n",
      "Epoch 5/8\n",
      "75110/75110 [==============================] - 43s - loss: 1.7645    - ETA:\n",
      "Epoch 6/8\n",
      "75110/75110 [==============================] - 43s - loss: 1.7352    \n",
      "Epoch 7/8\n",
      "75110/75110 [==============================] - 43s - loss: 1.7123    \n",
      "Epoch 8/8\n",
      "75110/75110 [==============================] - 48s - loss: 1.6927    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x112737610>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can implement a GRU in Keras by just replacing `SimpleRNN` with `GRU`:\n",
    "model = Sequential([\n",
    "            GRU(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n",
    "                   activation='relu', inner_init='identity'),\n",
    "            TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "        ])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "model.fit(oh_x_rnn, oh_y_rnn, batch_size = 64, nb_epoch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', 's', ' ', 'p', 's', ' ']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_nexts_oh(inp):\n",
    "    idxs = np.array([char_indices[c] for c in inp])\n",
    "    arr = to_categorical(idxs, vocab_size)\n",
    "    p = model.predict(arr[np.newaxis,:])[0]\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]\n",
    "\n",
    "get_nexts_oh(' this is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# V. Theano GRU\n",
    "\n",
    "### Separate Weights\n",
    "\n",
    "The Theano GRU looks like like the simple Theano RNN, except for the use of the reset and update gates. Each of these gates requires its own hidden & input weights, so we add those to our weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_h = id_and_bias(n_hidden)\n",
    "W_x = init_wgts(n_input, n_hidden)\n",
    "W_y = wgts_and_bias(n_hidden, n_output)\n",
    "\n",
    "# reset gate NN weights\n",
    "rW_h = init_wgts(n_hidden, n_hidden)\n",
    "rW_x = wgts_and_bias(n_input, n_hidden)\n",
    "\n",
    "# update gate NN weights\n",
    "uW_h = init_wgts(n_hidden, n_hidden)\n",
    "uW_x = wgts_and_bias(n_input, n_hidden)\n",
    "\n",
    "w_all = list(chain.from_iterable([W_h, W_y, uW_x, rW_x]))\n",
    "w_all.extend([W_x, uW_h, rW_h])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a Gate - it's just a sigmoid applied to the addition of the Dot Products of the input vectors.\n",
    "\n",
    "Input: inputs, hidden state, hidden state weights, input weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gate(x, h, W_h, W_x, b_x):\n",
    "    return nnet.sigmoid(T.dot(x, W_x) + b_x + T.dot(h, W_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Step function is nearly identical to before, except that it multiplies the hidden state by the reset gate, and updates the hidden state based on the update gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(x, h, W_h, b_h, W_y, b_y, \n",
    "         uW_x, ub_x, rW_x, rb_x, W_x, uW_h, rW_h):\n",
    "    reset = gate(x, h, rW_h, rW_x, rb_x) # reset gate is little NN\n",
    "    update = gate(x, h, uW_h, uW_x, ub_x)\n",
    "    h_new = gate(x, h * reset, W_h, W_x, b_h) # hidden states X by reset gate\n",
    "    h = update * h + (1 - update) * h_new # actual new hidden state\n",
    "    y = nnet.softmax(T.dot(h, W_y) + b_y) # output = hidden state X hidden weight matrix + the biases\n",
    "    return h, T.flatten(y, 1)\n",
    "# update gate decides how much h_new will replace h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there're these reset & update gates, the GRU-NN has the ability to learn these special sets of weights `(rW_h, uW_h, ...)`, to make sure it throws away or keeps more state when that's a good idea.\n",
    "\n",
    "Everything here onwards is identical to the simple Theano RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:27.05\n",
      "Error:22.67\n",
      "Error:22.10\n",
      "Error:21.20\n",
      "Error:20.29\n",
      "Error:20.63\n",
      "Error:20.32\n",
      "Error:19.72\n",
      "Error:19.52\n",
      "Error:19.77\n",
      "Error:19.02\n",
      "Error:19.13\n",
      "Error:19.84\n",
      "Error:18.95\n",
      "Error:18.41\n",
      "Error:19.56\n",
      "Error:19.29\n",
      "Error:18.93\n",
      "Error:18.32\n",
      "Error:18.15\n",
      "Error:17.89\n",
      "Error:17.86\n",
      "Error:18.34\n",
      "Error:17.85\n",
      "Error:18.16\n",
      "Error:17.97\n",
      "Error:17.64\n",
      "Error:17.76\n",
      "Error:17.77\n",
      "Error:18.00\n",
      "Error:18.23\n",
      "Error:17.87\n",
      "Error:18.19\n",
      "Error:17.78\n",
      "Error:17.61\n",
      "Error:18.20\n",
      "Error:17.41\n",
      "Error:17.98\n",
      "Error:17.60\n",
      "Error:17.72\n",
      "Error:17.00\n",
      "Error:17.44\n",
      "Error:17.27\n",
      "Error:17.57\n",
      "Error:17.59\n",
      "Error:17.67\n",
      "Error:17.39\n",
      "Error:18.62\n",
      "Error:17.41\n",
      "Error:17.69\n",
      "Error:17.00\n",
      "Error:17.46\n",
      "Error:16.78\n",
      "Error:16.83\n",
      "Error:17.55\n",
      "Error:17.27\n",
      "Error:16.93\n",
      "Error:17.37\n",
      "Error:17.27\n",
      "Error:17.12\n",
      "Error:16.82\n",
      "Error:17.29\n",
      "Error:17.12\n",
      "Error:17.13\n",
      "Error:16.82\n",
      "Error:16.83\n",
      "Error:16.68\n",
      "Error:16.72\n",
      "Error:17.29\n",
      "Error:16.67\n",
      "Error:17.31\n",
      "Error:16.76\n",
      "Error:16.56\n",
      "Error:16.51\n",
      "Error:16.48\n"
     ]
    }
   ],
   "source": [
    "[v_h, v_y], _ = theano.scan(step, sequences=t_inp,\n",
    "                            outputs_info=[t_h0, None], non_sequences=w_all)\n",
    "error = nnet.categorical_crossentropy(v_y, t_outp).sum()\n",
    "g_all = T.grad(error, w_all)\n",
    "\n",
    "upd = upd_dict(w_all, g_all, lr)\n",
    "fn = theano.function(all_args, error, updates=upd, allow_input_downcast=True)\n",
    "\n",
    "err = 0.0; l_rate = 0.1\n",
    "for i in xrange(len(X)):\n",
    "    err += fn(np.zeros(n_hidden), X[i], Y[i], l_rate)\n",
    "    if i % 1000 == 999:\n",
    "        l_rate *= 0.95\n",
    "        print(\"Error:{:.2f}\".format(err/1000))\n",
    "        err = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Weights\n",
    "\n",
    "The previous section can be made simpler and faster by concatenating the hidden & input matrices and inputs together. This section's identical to the previous except for this concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = (shared(np.concatenate([np.eye(n_hidden), normal(size=(n_input, n_hidden))])\n",
    "            .astype(np.float32)), init_bias(n_hidden))\n",
    "\n",
    "rW = wgts_and_bias(n_input + n_hidden, n_hidden)\n",
    "uW = wgts_and_bias(n_input + n_hidden, n_hidden)\n",
    "W_y = wgts_and_bias(n_hidden, n_output)\n",
    "w_all = list(chain.from_iterable([W, W_y, uW, rW]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gate(m, W, b): return nnet.sigmoid(T.dot(m, W) + b)\n",
    "\n",
    "def step(x, h, W, b, W_y, b_y, uW, ub, rW, rb):\n",
    "    m = T.concatenate([h, x])\n",
    "    reset = gate(m, rW, rb)\n",
    "    update = gate(m, uW, ub)\n",
    "    m = T.concatenate([h * reset, x])\n",
    "    h_new = gate(m, W, b)\n",
    "    h = update * h + (1 - update) * h_new\n",
    "    y = nnet.softmax(T.dot(h, W_y) + b_y)\n",
    "    return h, T.flatten(y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[v_h, v_y], _ = theano.scan(step, sequences=t_inp,\n",
    "                            outputs_info=[t_h0, None], non_sequences=w_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upd_dict(wgts, grads, lr):\n",
    "    return OrderedDict({w:w-g*lr for (w,g) in zip(wgts, grads)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = nnet.categorical_crossentropy(v_y, t_outp).sum()\n",
    "g_all = T.grad(error, w_all)\n",
    "\n",
    "upd = upd_dict(w_all, g_all, lr)\n",
    "fn = theano.function(all_args, error, updates=upd, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:24.76\n",
      "Error:22.17\n",
      "Error:22.01\n",
      "Error:21.26\n",
      "Error:20.48\n",
      "Error:21.02\n",
      "Error:20.70\n",
      "Error:20.18\n",
      "Error:19.97\n",
      "Error:20.30\n",
      "Error:19.59\n",
      "Error:19.72\n",
      "Error:20.30\n",
      "Error:19.58\n",
      "Error:19.04\n",
      "Error:19.96\n",
      "Error:19.72\n",
      "Error:19.63\n",
      "Error:19.00\n",
      "Error:18.87\n",
      "Error:18.50\n",
      "Error:18.56\n",
      "Error:19.07\n",
      "Error:18.50\n",
      "Error:18.79\n",
      "Error:18.58\n",
      "Error:18.35\n",
      "Error:18.36\n",
      "Error:18.33\n",
      "Error:18.48\n",
      "Error:18.83\n",
      "Error:18.38\n",
      "Error:18.61\n",
      "Error:18.27\n",
      "Error:17.98\n",
      "Error:18.58\n",
      "Error:17.83\n",
      "Error:18.42\n",
      "Error:17.92\n",
      "Error:18.03\n",
      "Error:17.37\n",
      "Error:17.77\n",
      "Error:17.63\n",
      "Error:17.97\n",
      "Error:17.82\n",
      "Error:17.88\n",
      "Error:17.66\n",
      "Error:17.82\n",
      "Error:17.68\n",
      "Error:17.79\n",
      "Error:17.15\n",
      "Error:17.36\n",
      "Error:16.71\n",
      "Error:16.80\n",
      "Error:17.43\n",
      "Error:17.27\n",
      "Error:16.76\n",
      "Error:17.31\n",
      "Error:17.05\n",
      "Error:16.90\n",
      "Error:16.69\n",
      "Error:17.11\n",
      "Error:16.94\n",
      "Error:16.73\n",
      "Error:16.53\n",
      "Error:16.55\n",
      "Error:16.27\n",
      "Error:16.49\n",
      "Error:17.04\n",
      "Error:16.42\n",
      "Error:16.80\n",
      "Error:16.36\n",
      "Error:16.21\n",
      "Error:16.15\n",
      "Error:16.10\n"
     ]
    }
   ],
   "source": [
    "err=0.0; l_rate=0.01\n",
    "for i in xrange(len(X)):\n",
    "    err += fn(np.zeros(n_hidden), X[i], Y[i], l_rate)\n",
    "    if i % 1000 == 999:\n",
    "        print (\"Error:{:.2f}\".format(err/1000))\n",
    "        err=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', 'n', '?', ' ', 'I', 's']\n",
      "['h', 'e', ' ', ' ', ' ', 's', 't', ' ']\n"
     ]
    }
   ],
   "source": [
    "f_y = theano.function([t_h0, t_inp], v_y, allow_input_downcast=True)\n",
    "pred = np.argmax(f_y(np.zeros(n_hidden), X[6]), axis=1)\n",
    "act = np.argmax(X[6], axis=1)\n",
    "\n",
    "actual = [indices_char[o] for o in act]\n",
    "prediction = [indices_char[o] for o in pred]\n",
    "\n",
    "print(actual)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
