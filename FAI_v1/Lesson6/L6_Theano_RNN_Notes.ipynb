{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wayne H Nixalo - 18 June 2017\n",
    "\n",
    "CodeAlong of Lesson 6 at 1:29:00 onwards [link](https://youtu.be/ll9y1U0SoVY?t=5376) --- note: this JNB not meant to be run.\n",
    "\n",
    "Coding in Theano is different to coding in Python because Theano's job in life is to provide a way for you to describe a computation, and compile it for the GPU and run it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = vocab_size\n",
    "n_output = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_wgts(rows, cols):\n",
    "    scale = math.sqrt(2/rows)\n",
    "    return shared(normal(scale=scale, size=(rwos, cols)).astype(np.float32))\n",
    "def init_bias(rows):\n",
    "    vec = np.zeros(rows, dtype=np.float32)\n",
    "    return shared(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our hidden weights (the arrow in the diagram which loops back to its matrix), we initialize it using an identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wgts_and_bias(n_in, n_out):\n",
    "    return init_wgts(n_in, n_out), init_bias(n_out)\n",
    "def id_and_bias(n):\n",
    "    return shared(np.eye(n, dtype=np.float32)), init_bias(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to build up a computation-graph, a series of steps saying 'in the future I'm going to give you some data, and when I do I want you to do these steps...'\n",
    "\n",
    "So we start off by describing the types of data we'll give it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll give it some input data\n",
    "t_inp = T.matrix('inp')\n",
    "# some output data\n",
    "t_outp = T.matrix('outp')\n",
    "# give it some way of initializing the first hidden state\n",
    "t_h0 = T.vector('h0')\n",
    "# also give it a learning rate which we can change later\n",
    "lr = T.scalar('lr')\n",
    "\n",
    "# we create a list of all args we provide to Theano later\n",
    "all_args = [t_h0, t_inp, t_outp, lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the weights & biases, up above we have a function called ```wgts_and_bias(..)``` in which we tell Theano the size of the matrix we want to create.\n",
    "\n",
    "The matrix that goes from input to hidden has ```n_input``` rows and ```n_hidden``` collumns.\n",
    "\n",
    "```wgts_and_bias``` returns a tuple of weights and biases.\n",
    "\n",
    "To create the weights, we first calculate the Glorot number, sqrt(2/n) -- the scale of the random numbers we're going to use -- then we create those random numbers using the Numpy ```normal(..)``` random number function, and then we use a special Theano keyword called ```shared```. ```shared(..)``` tells Theano that the data inside is something we want it to pass off to the GPU later and keep track of.\n",
    "\n",
    "So once you wrap something in ```shared```, it kind of belongs to Theano now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights & bias to hidden layer\n",
    "W_h = id_and_bias(b_hidden)\n",
    "# weights & bias to input\n",
    "W_x = wgts_and_bias(n_input, n_hidden)\n",
    "# weights & bias to output\n",
    "W_y = wgts_and_bias(n_hidden, n_output)\n",
    "# stick all manually constructed weight matrices and bias vectors \n",
    "# in a list:\n",
    "w_all = list(chain.from_iterable([W_h, W_x, W_y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a function, ```chain.from_iterable(..)```, that takes a list of tuples and turns them into one big list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we have to do is tell Theano what happens each time we take a single step of this RNN.\n",
    "\n",
    "There's no such thing as a for-loop on a GPU bc the GPU is built to & wants to parallelize things & do multiple things at the same time.\n",
    "\n",
    "There is something very similar to a for-loop that you can parallelize, it's called a ```scan``` operation.\n",
    "\n",
    "A ```scan``` operation is smth where you call some function (```step```) for every element (```t_inp```) of some sequence, and at every point the function returns some output, and the next time thru that function is called, it gets the output of the previous time to you called it along with the next element of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of scan:\n",
    "def scan(fn, start, seq):\n",
    "    res = [] # array of results\n",
    "    prev = start\n",
    "    for s in seq:\n",
    "        app = fn(prev, s) # apply function to previous result & next elem in sequence\n",
    "        res.append(app)\n",
    "        prev = app\n",
    "    return res\n",
    "\n",
    "scan(lambda prev,curr: prev+curr, 0, range(5))\n",
    "\n",
    "# output:\n",
    "# [0, 1, 3, 6, 10]\n",
    "\n",
    "# the scan operation defines a cumulative sum.\n",
    "\n",
    "# It is possible to write a parallel version of this.\n",
    "# If you can turn you algorithm into a scan, you can run it quickly \n",
    "# on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fucntion we'll call on ea. step thru is a fn called ```step```. ```step``` takes the input, ```x```, does a dot-product by the weight-matrix we created earlier, ```W_x```, adds on the bias term we created earlier, ```b_x```. then we do the same thing, taking our previous hidden state ```h```, multiplying it by the hidden weight-matrix ```W_h``` and adding the biases ```b_h```; then putting that whole thing through the activation function, ```nnet.relu```.\n",
    "\n",
    "After we do that we want to create an output each time. Output is exactly the same thing. It takes the result of ```h```, the hidden state, multiply it by the outputs weight-vector, adding on the bias; and this time we use ```nnnet.softmax```.\n",
    "\n",
    "At the end of that, we return the hidden state we have so far, and the output, ```T.flatten(y, 1)```\n",
    "\n",
    "All that happens each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(x, h, W_h, b_h, W_x, b_x, W_y, b_y):\n",
    "    h = nnet.relu(T.dot(x, W_x) + b_x + T.dot(h, W_h) + b_h)\n",
    "    y = nnet.softmax(T.dot(h, W_y) + b_y)\n",
    "    return h, T.flatten(y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sequence we're passing into it -- we're just describing a computation here, so we tell Theano it will will be a matrix.\n",
    "\n",
    "For the starting point, ```outputs_info=[t_h0, None]```, we tell Theano via ```t_h0 = T.vector('h0')``` we'll provide it an initial hidden state.\n",
    "\n",
    "Finally, in Theano you have to tell it what all the other things you'll pass to the function, and we'll pass it the whole list of weights we created up above via: ```non_sequences=w_all```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[v_h, v_y], = theano.scan(step, sequences=t_inp,\n",
    "                          outputs_info=[t_h0, None], non_sequences=w_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point we've described how to execute a whole sequence of stesp for an RNN. We haven't given it any data to do it; we've just set up the computation.\n",
    "\n",
    "When that computation is run, it's going to return 2 things bc ```step``` returns 2 things: the hidden state ```v_h```, and our output activations ```v_y```.\n",
    "\n",
    "Now we need to calculate our error. Using cat-crossent we compare the output of our scan, ```v_y``` to some matrix ```t_outp```. Then add it all together.\n",
    "\n",
    "We want to apply SGD after every step, meaning we have to take the derivative of w_all wrt. all the weights, and use that along with the learning rate to update all the weights. Theano has a simple function call to do that: ```T.grad(..)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = nnet.categorical_crossentropy(v_y, t_outp).sum()\n",
    "g_all = T.grad(error, w_all)\n",
    "# \"please tell me the gradient, `g_all`, of the function `error`, \n",
    "# wrt. these gradients: `w_all`\"\n",
    "# Theano will symbolically calculate all the derivatives for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready now to build our final function. It takes as input, all our arguments, ```all_args```. It'll create the ```error``` as its output. At each step it's going to do some updates via ```upd_dict```.\n",
    "\n",
    "What ```upd_dict``` does is create a dictionary that's going to map every noe of our weights to the weight ```w``` minus each one of our gradients ```g``` times the learning rate ```lr```.\n",
    "\n",
    "```w``` comes from wgts, ```g``` comes from ```grads```.\n",
    "\n",
    "So it updates every weight to itself minus its gradient times the learning rate.\n",
    "\n",
    "What Theano does via ```updates``` is it says: every time you calculate the next step, I want you to change your shared variables as follows: (and ```upd``` is the list of changes to make). \n",
    "\n",
    "And that's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upd_dict(w_all, g_all, lr):\n",
    "    return OrderedDict({w: w-g*lr for (w,g) in zip(wgts,grads)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upd = upd_dict(w_all, g_all, lr)\n",
    "fn = theano.function(all_args, error, updates=upd, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our one-hot encoded X's and Y's, and we have to manually create our own loop; Theano has nothing built-in for this.\n",
    "\n",
    "We go through every element of our input ```X```, and call that function that we just created above, and pass in all its inputs, ```all_args```, for initial hidden state ```t_h0```, input ```t_inp```, target ```t_outp```, and learning rate ```lr```.\n",
    "\n",
    "Initial hidden state, a bunch of zeros: ```np.zeros(n_hidden)```.\n",
    "\n",
    "The condition ```if i % 1000 == 999: ...``` just prints out the error every 1000 loops.\n",
    "\n",
    "*(we're using stochastic gradient descent with a minibatch size of 1)*\n",
    "\n",
    "*gradient descent w/o stochastic means your using a minibatch size of the whole dataset*\n",
    "\n",
    "*so this is 'online' gradient descent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = oh_x_rnn\n",
    "Y = oh_y_rnn\n",
    "# X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err=0.0; l_rate=0.01\n",
    "for i in range(len(X)):\n",
    "    err+=fn(np.zeros(n_hidden), X[i], Y[i], l_rate)\n",
    "    if i % 1000 == 999:\n",
    "        print(\"Error:{:.3f}\".format(err/1000))\n",
    "        err=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of learning (via the loop above), we create a new Theano fn which takes some piece of input along with some initial hidden state, and produce not the loss, but the output.\n",
    "\n",
    "This is to do testing: the fn goes from out inputs to our vector of outputs ```v_y```.\n",
    "\n",
    "Our predictions ```preds```, will be to take that fn ```f_y```, pass it our initial hidden state, ```np.zeros(n_hidden)```, and some input, say ```X[6]```, and that'll give us some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_y = theano.function([t_h0, t_inp], v_y, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(f_y(np.zeros(n_hidden), X[6]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grabbing sample input text\n",
    "act = np.argmax(X[6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# displaying input text\n",
    "[indices_char[o] for o in act]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# displaying expected outputs\n",
    "[indices_char[o] for o in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above 2 lines will show what the model expected to come after each character.\n",
    "\n",
    "In lecture:\n",
    "```\n",
    "act:  ['t', 'h', 'e', 'n', '?', ' ', 'I', 's']\n",
    "---\n",
    "pred: ['h', 'e', ' ', ' ', ' ', 'T', 'n', ' ']\n",
    "```\n",
    "And that's building a Recurrent Neural Network from Scratch using Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
